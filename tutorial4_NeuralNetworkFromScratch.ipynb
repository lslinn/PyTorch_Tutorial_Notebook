{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we will learn to build a simple neural network from scratch.\n",
        "When I say \"from scratch,\" it means we will not use PyTorch's `torch.nn` module (which is designed to easily construct neural networks).\n",
        "Instead, we will manually build the layers and connections using basic matrix operations.\n",
        "\n",
        "**Why not `torch.nn`?**\n",
        "- The `torch.nn` module is the most common and recommended way to build neural networks in PyTorch. It simplifies the process by providing pre-built layers and functionality, allowing you to focus on designing and training the model.\n",
        "- By using `torch.nn`, you can quickly create complex models without worrying about low-level details like manually implementing matrix multiplications or tracking gradients, which is why it’s the preferred approach in most real-world applications.\n",
        "\n",
        "**Why build from scratch?**\n",
        "- Understanding how neural networks work under the hood is essential for developing a deep understanding of how each layer, activation function, and weight update interacts.\n",
        "- This exercise will give us a solid foundation to understand what’s really happening behind `torch.nn`, and it will also give us the ability to design advanced neural network architectures that may not be readily available in `torch.nn`.\n",
        "\n",
        "If you’re focused on getting up to speed with PyTorch quickly, feel free to skip the beginning of this tutorial and jump straight to the section on building a neural network with torch.nn, starting after the line `import torch.nn as nn`. (And don’t forget to check the last part of this tutorial, *Convention of the Input/Output Tensor Shapes (IMPORTANT)*)."
      ],
      "metadata": {
        "id": "WOeUfqp-WB3o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Nothing is really complicated**\n",
        "A fully connected neural network (also known as a dense layer) follows the mathematical expression:\n",
        "\n",
        "$$a_i^{(l)} = \\sum_{j=1}^{n_{l-1}} w_{ij}^{(l)} a_j^{(l-1)} + b_i^{(l)}$$\n",
        "\n",
        "where:\n",
        "- $a_i^{(l)}$ represents the activation/output of the $i$-th neuron in layer $l$,\n",
        "- $w_{ij}^{(l)}$ is the weight connecting the $j$-th neuron in layer $(l-1)$ to the $i$-th neuron in layer $l$,\n",
        "- $b_i^{(l)}$ is the bias term for the $i$-th neuron in layer $l$,\n",
        "- and $a_j^{(l-1)}$ is the activation/output of the $j$-th neuron in the previous layer $(l-1)$.\n",
        "\n",
        "You can see that the operations involved are just matrix multiplication and addition. Specifically, multiplying the weight matrix $W^{(l)}$ by the output vector $a^{(l-1)}$, and then adding the bias vector $b^{(l)}$.\n",
        "\n",
        "Note that the connections between layers here are linear. No activation function has been applied yet. (we will talk about activation functions later)"
      ],
      "metadata": {
        "id": "e1_RPDPtcfKx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5TDFFQyaVrvW",
        "outputId": "aab97da1-6414-49cf-e961-472fdfcb8f0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 1]) torch.Size([1, 3]) torch.Size([3, 3]) torch.Size([3]) torch.Size([3])\n",
            "Prediction before training: model([5]) = tensor([[ 0.0793, -0.0141]], grad_fn=<AddBackward0>)\n",
            "Epoch 1, loss = 48.60699844\n",
            "Epoch 11, loss = 35.05459213\n",
            "Epoch 21, loss = 0.61255360\n",
            "Epoch 31, loss = 0.49195898\n",
            "Epoch 41, loss = 0.39488536\n",
            "Epoch 51, loss = 0.31532276\n",
            "Epoch 61, loss = 0.25029284\n",
            "Epoch 71, loss = 0.19739500\n",
            "Epoch 81, loss = 0.15463699\n",
            "Epoch 91, loss = 0.12032990\n",
            "Epoch 101, loss = 0.09302635\n",
            "Epoch 111, loss = 0.07148015\n",
            "Epoch 121, loss = 0.05462322\n",
            "Epoch 131, loss = 0.04154541\n",
            "Epoch 141, loss = 0.03148174\n",
            "Epoch 151, loss = 0.02379571\n",
            "Epoch 161, loss = 0.01796615\n",
            "Epoch 171, loss = 0.01357216\n",
            "Epoch 181, loss = 0.01027791\n",
            "Epoch 191, loss = 0.00781928\n",
            "Prediction after training: model([5]) = tensor([[ 9.8465, 14.9062]], grad_fn=<AddBackward0>)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Creating dataset in PyTorch tensors\n",
        "X = torch.tensor([[1], [2], [3], [4]], dtype=torch.float32)  # shape (4, 1)\n",
        "Y = torch.tensor([[2, 3], [4, 6], [6, 9], [8, 12]], dtype=torch.float32)  # shape (4, 2)\n",
        "# IMPORTANT: Why do we change the shape of tensors?\n",
        "# --> go to the end of this tutorial: Convention of the Input/Output Tensor Shapes\n",
        "\n",
        "\"\"\"\n",
        "We shall use object-oriented programming (OOP) to construct neural networks using a class.\n",
        "(Though using simple functions is also possible.)\n",
        "\n",
        "In OOP, a class acts as a blueprint for creating objects,\n",
        "allowing us to define attributes and methods that operate on these attributes.\n",
        "(It’s a fundamental concept in Python programming.)\n",
        "\"\"\"\n",
        "\n",
        "class NN:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        # Initialize weights and biases manually\n",
        "        self.mean = 0.\n",
        "        self.std_dev = 0.1\n",
        "        self.w1 = torch.normal(mean=self.mean, std=self.std_dev, size=(input_size, hidden_size), requires_grad=True)\n",
        "        self.b1 = torch.normal(mean=self.mean, std=self.std_dev, size=(hidden_size,), requires_grad=True)\n",
        "        self.w2 = torch.normal(mean=self.mean, std=self.std_dev, size=(hidden_size, hidden_size), requires_grad=True)\n",
        "        self.b2 = torch.normal(mean=self.mean, std=self.std_dev, size=(hidden_size,), requires_grad=True)\n",
        "        self.w_out = torch.normal(mean=self.mean, std=self.std_dev, size=(hidden_size, output_size), requires_grad=True)\n",
        "        self.b_out = torch.normal(mean=self.mean, std=self.std_dev, size=(output_size,), requires_grad=True)\n",
        "\n",
        "    # forward ver1\n",
        "    def forward(self, x):\n",
        "        # Input layer (input to hidden1)\n",
        "        z1 = x @ self.w1 + self.b1  # matrix multiplication + bias\n",
        "        # First hidden layer to second hidden layer (hidden1 to hidden2)\n",
        "        z2 = z1 @ self.w2 + self.b2  # matrix multiplication + bias\n",
        "        # Second layer to output (hidden2 to output)\n",
        "        z_out = z2 @ self.w_out + self.b_out  # matrix multiplication + bias\n",
        "        return z_out\n",
        "\n",
        "    \"\"\"\n",
        "    Instead of \"@\" and \"+\", you can use `torch.matmul()` and `torch.add()` as well.\n",
        "    They are the same. (see forward ver2 below)\n",
        "    \"\"\"\n",
        "    # # forward ver2\n",
        "    # def forward(self, x):\n",
        "    #     # Input layer (input to hidden1)\n",
        "    #     z1 = torch.add(torch.matmul(x, self.w1), self.b1)\n",
        "    #     # First hidden layer to second hidden layer (hidden1 to hidden2)\n",
        "    #     z2 = torch.add(torch.matmul(z1, self.w2), self.b2)\n",
        "    #     # Second layer to output (hidden2 to output)\n",
        "    #     z_out = torch.add(torch.matmul(z2, self.w_out), self.b_out)\n",
        "    #     return z_out\n",
        "\n",
        "    def parameters(self):\n",
        "        # Collect all parameters for easy access (useful for optimization)\n",
        "        return [self.w1, self.b1, self.w2, self.b2, self.w_out, self.b_out]\n",
        "\n",
        "# Fix input and output size based on the shape of X and Y\n",
        "model = NN(input_size=X.shape[1], hidden_size=3, output_size=Y.shape[1])\n",
        "# (batch size, number of features) @ (number of features, layer1 dimension) @ (layer1 dimensiton, layer2 dimesion) @ (layer2 dimension, output size)\n",
        "# (4, 1) @ (1, 3) @ (3, 3) @ (3, 2) --> so the output will be a tensor of size (4, 2) \"4 set of output data (batch size) each with 2 output features\"\n",
        "\n",
        "# Checking the matrix multiplication's shape\n",
        "print(X.shape, model.w1.shape, model.w2.shape, model.b1.shape, model.b2.shape)\n",
        "\n",
        "# Mean squared error (MSE) loss function\n",
        "def loss(y, y_predicted):\n",
        "    return ((y_predicted - y) ** 2).mean()\n",
        "\n",
        "# Initial prediction\n",
        "print(f'Prediction before training: model([5]) = {model.forward(torch.tensor([[5]], dtype=torch.float32))}')\n",
        "\n",
        "# Main training loop settings\n",
        "learning_rate = 0.01\n",
        "n_iterations = 200\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for epoch in range(n_iterations):\n",
        "    # Forward pass\n",
        "    y_pred = model.forward(X)\n",
        "\n",
        "    # Calculate loss\n",
        "    l = loss(Y, y_pred)\n",
        "\n",
        "    # Zero gradients:: Putting zero_grad before backward is also fine!\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Backward pass to compute gradients\n",
        "    l.backward()\n",
        "\n",
        "    # Optimizer step\n",
        "    optimizer.step()\n",
        "\n",
        "    # Print training info\n",
        "    if epoch % 10 == 0:\n",
        "        print(f'Epoch {epoch + 1}, loss = {l:.8f}')\n",
        "\n",
        "# Prediction after training\n",
        "print(f'Prediction after training: model([5]) = {model.forward(torch.tensor([[5]], dtype=torch.float32))}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# You can also print the model parameters simply with:\n",
        "print(f'model parameters = {model.parameters()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvNItLpYv97f",
        "outputId": "53a0d97b-ec49-4a9a-e710-556373853a5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model parameters = [tensor([[ 1.0811, -0.7628, -0.6782]], requires_grad=True), tensor([-0.1405,  0.3181,  0.1594], requires_grad=True), tensor([[-0.1280,  0.4199, -1.0095],\n",
            "        [ 0.2909, -0.1669,  0.7717],\n",
            "        [ 0.0541, -0.1634,  0.6986]], requires_grad=True), tensor([ 0.0021,  0.0882, -0.1247], requires_grad=True), tensor([[-0.0182, -0.3003],\n",
            "        [ 0.1421,  0.4845],\n",
            "        [-0.8439, -1.1604]], requires_grad=True), tensor([0.5553, 0.6318], requires_grad=True)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Or in a fancier way:\n",
        "def print_model_parameters(model):\n",
        "    for name, param in zip(['w1', 'b1', 'w2', 'b2'], model.parameters()):\n",
        "        print(f'Parameter name: {name}')\n",
        "        print(f'Value: \\n{param.data}')\n",
        "        print(f'Gradient: \\n{param.grad}')\n",
        "        print('---')\n",
        "\n",
        "print_model_parameters(model) # note that the grad printed here are all none since"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XXK5BNm2akT",
        "outputId": "98b5fe52-00a0-4666-ca93-bd04ffea49f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter name: w1\n",
            "Value: \n",
            "tensor([[ 1.0811, -0.7628, -0.6782]])\n",
            "Gradient: \n",
            "tensor([[-0.0049,  0.0037,  0.0031]])\n",
            "---\n",
            "Parameter name: b1\n",
            "Value: \n",
            "tensor([-0.1405,  0.3181,  0.1594])\n",
            "Gradient: \n",
            "tensor([ 0.0668, -0.0500, -0.0439])\n",
            "---\n",
            "Parameter name: w2\n",
            "Value: \n",
            "tensor([[-0.1280,  0.4199, -1.0095],\n",
            "        [ 0.2909, -0.1669,  0.7717],\n",
            "        [ 0.0541, -0.1634,  0.6986]])\n",
            "Gradient: \n",
            "tensor([[ 0.0022, -0.0041,  0.0125],\n",
            "        [-0.0031,  0.0065, -0.0216],\n",
            "        [-0.0019,  0.0037, -0.0120]])\n",
            "---\n",
            "Parameter name: b2\n",
            "Value: \n",
            "tensor([ 0.0021,  0.0882, -0.1247])\n",
            "Gradient: \n",
            "tensor([-0.0074,  0.0163, -0.0585])\n",
            "---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "moving away from scratch\n",
        "--> Building Neural Networks with `torch.nn`\n",
        "\n",
        "From here, we explore `torch.nn` for building neural networks\n",
        "that may become your go-to approach\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "# Creating dataset in PyTorch tensors\n",
        "X = torch.tensor([[1], [2], [3], [4]], dtype=torch.float32)  # shape (4, 1)\n",
        "Y = torch.tensor([[2, 3], [4, 6], [6, 9], [8, 12]], dtype=torch.float32)  # shape (4, 2)\n",
        "\n",
        "class NN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(NN, self).__init__()\n",
        "        # Define layers using nn.Linear (includes weights and biases automatically)\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)  # First layer: input to hidden1\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)  # Second layer: hidden1 to hidden2\n",
        "        self.fc_out = nn.Linear(hidden_size, output_size) # output layer: hidden2 to output\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply the layers in sequence (no activation function for simplicity)\n",
        "        x = self.fc1(x)  # input -> First layer\n",
        "        x = self.fc2(x)  # First layer -> Second layer\n",
        "        x = self.fc_out(x) # Second layer -> output\n",
        "        return x\n",
        "    \"\"\"\n",
        "    Note:\n",
        "    1.  __init__ Method:\n",
        "\t•\tThe __init__ method is the constructor of the class. It is used to define and initialize the layers of the neural network.\n",
        "\t•\tIn this case, we define three fully connected layers (nn.Linear), which include weights and biases automatically.\n",
        "\t•\tThe method receives input_size, hidden_size, and output_size to configure the dimensions of the layers.\n",
        "\t2.\tsuper(NN, self).__init__():\n",
        "\t•\tThis line calls the __init__ method of the parent class nn.Module.\n",
        "\t•\tIt’s necessary because NN inherits from nn.Module, and calling super() ensures that all the internal features of nn.Module (like parameter registration, autograd, etc.) are correctly initialized.\n",
        "\t•\tWithout this line, the class wouldn’t function properly as a PyTorch model.\n",
        "    \"\"\"\n",
        "\n",
        "# Fix input and output size based on the shape of X and Y\n",
        "model = NN(input_size=X.shape[1], hidden_size=3, output_size=Y.shape[1])\n",
        "\n",
        "# Mean squared error (MSE) loss function\n",
        "def loss(y, y_predicted):\n",
        "    return ((y_predicted - y) ** 2).mean()\n",
        "\n",
        "# Initial prediction\n",
        "print(f'Prediction before training: model([5]) = {model.forward(torch.tensor([[5]], dtype=torch.float32))}')\n",
        "\n",
        "# Main training loop settings\n",
        "learning_rate = 0.01\n",
        "n_iterations = 200\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for epoch in range(n_iterations):\n",
        "    # Forward pass\n",
        "    y_pred = model.forward(X)\n",
        "\n",
        "    # Calculate loss\n",
        "    l = loss(Y, y_pred)\n",
        "\n",
        "    # Zero gradients:: Putting zero_grad before backward is also fine!\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Backward pass to compute gradients\n",
        "    l.backward()\n",
        "\n",
        "    # Optimizer step\n",
        "    optimizer.step()\n",
        "\n",
        "    # Print training info\n",
        "    if epoch % 10 == 0:\n",
        "        print(f'Epoch {epoch + 1}, loss = {l:.8f}')\n",
        "\n",
        "# Prediction after training\n",
        "print(f'Prediction after training: model([5]) = {model.forward(torch.tensor([[5]], dtype=torch.float32))}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YnrgEfHC2uVV",
        "outputId": "7e676547-004e-44f9-964e-9dc356bad3f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction before training: model([5]) = tensor([[-0.7878,  0.5648]], grad_fn=<AddmmBackward0>)\n",
            "Epoch 1, loss = 48.46225739\n",
            "Epoch 11, loss = 0.22184399\n",
            "Epoch 21, loss = 0.00322931\n",
            "Epoch 31, loss = 0.00308042\n",
            "Epoch 41, loss = 0.00294041\n",
            "Epoch 51, loss = 0.00280765\n",
            "Epoch 61, loss = 0.00268153\n",
            "Epoch 71, loss = 0.00256158\n",
            "Epoch 81, loss = 0.00244738\n",
            "Epoch 91, loss = 0.00233854\n",
            "Epoch 101, loss = 0.00223475\n",
            "Epoch 111, loss = 0.00213572\n",
            "Epoch 121, loss = 0.00204117\n",
            "Epoch 131, loss = 0.00195090\n",
            "Epoch 141, loss = 0.00186468\n",
            "Epoch 151, loss = 0.00178229\n",
            "Epoch 161, loss = 0.00170357\n",
            "Epoch 171, loss = 0.00162833\n",
            "Epoch 181, loss = 0.00155642\n",
            "Epoch 191, loss = 0.00148770\n",
            "Prediction after training: model([5]) = tensor([[10.0701, 14.9532]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# With torch.nn.Module, our model now has a named_parameters() function\n",
        "# which returns both the names and the values of the model's parameters.\n",
        "def print_model_parameters(model):\n",
        "    for name, param in model.named_parameters():\n",
        "        print(f'Parameter name: {name}')\n",
        "        print(f'Value: \\n{param.data}')\n",
        "        print(f'Gradient: \\n{param.grad}')\n",
        "        print('---')\n",
        "\n",
        "print_model_parameters(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBxxtozm3RVb",
        "outputId": "e97e49fc-1d49-4965-d1b0-6dda54419011"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter name: fc1.weight\n",
            "Value: \n",
            "tensor([[ 1.1824],\n",
            "        [-1.3786],\n",
            "        [ 0.5873]])\n",
            "Gradient: \n",
            "tensor([[ 1.0512e-04],\n",
            "        [-5.6312e-06],\n",
            "        [-2.1590e-04]])\n",
            "---\n",
            "Parameter name: fc1.bias\n",
            "Value: \n",
            "tensor([-0.6605, -0.1920,  0.0790])\n",
            "Gradient: \n",
            "tensor([ 0.0004,  0.0004, -0.0013])\n",
            "---\n",
            "Parameter name: fc2.weight\n",
            "Value: \n",
            "tensor([[ 0.7895, -0.7183,  0.1394],\n",
            "        [ 0.1031, -0.9238,  0.5506],\n",
            "        [-0.1248, -0.2326, -0.0709]])\n",
            "Gradient: \n",
            "tensor([[-0.0005, -0.0006,  0.0003],\n",
            "        [ 0.0011,  0.0010, -0.0004],\n",
            "        [-0.0016, -0.0016,  0.0007]])\n",
            "---\n",
            "Parameter name: fc2.bias\n",
            "Value: \n",
            "tensor([-0.3280, -0.3290, -0.0565])\n",
            "Gradient: \n",
            "tensor([ 0.0013, -0.0023,  0.0036])\n",
            "---\n",
            "Parameter name: fc_out.weight\n",
            "Value: \n",
            "tensor([[ 0.5222,  0.5795, -0.0590],\n",
            "        [ 0.8826,  0.6873,  0.1868]])\n",
            "Gradient: \n",
            "tensor([[ 0.0065, -0.0026, -0.0017],\n",
            "        [-0.0043,  0.0018,  0.0012]])\n",
            "---\n",
            "Parameter name: fc_out.bias\n",
            "Value: \n",
            "tensor([0.3632, 0.7999])\n",
            "Gradient: \n",
            "tensor([-0.0193,  0.0129])\n",
            "---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Additionally, we sometimes see models with simpler NN built using `nn.Sequential`:\n",
        "\"\"\"\n",
        "\n",
        "X = torch.tensor([[1], [2], [3], [4]], dtype=torch.float32)  # shape (4, 1)\n",
        "Y = torch.tensor([[2, 3], [4, 6], [6, 9], [8, 12]], dtype=torch.float32)  # shape (4, 2)\n",
        "\n",
        "# Constructing the model directly with nn.Sequential\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(1, 3),  # input_size = 1, hidden_size = 3\n",
        "    nn.Linear(3, 3),  # hidden_size to hidden_size\n",
        "    nn.Linear(3, 2)   # hidden_size to output_size = 2\n",
        ")\n",
        "\n",
        "# Mean squared error (MSE) loss function\n",
        "def loss(y, y_predicted):\n",
        "    return ((y_predicted - y) ** 2).mean()\n",
        "\n",
        "# Main training loop settings\n",
        "learning_rate = 0.01\n",
        "n_iterations = 200\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for epoch in range(n_iterations):\n",
        "    # Forward pass\n",
        "    y_pred = model(X)\n",
        "\n",
        "    # Calculate loss\n",
        "    l = loss(Y, y_pred)\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer.zero_grad()\n",
        "    l.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Print training info\n",
        "    if epoch % 10 == 0:\n",
        "        print(f'Epoch {epoch + 1}, loss = {l:.8f}')\n",
        "\n",
        "# Prediction after training\n",
        "print(f'Prediction after training: model([5]) = {model.forward(torch.tensor([[5]], dtype=torch.float32))}')\n",
        "\n",
        "\"\"\"\n",
        "While `nn.Sequential` allows us to construct simple neural networks in a neat and compact way,\n",
        "it is limited in terms of flexibility. For more complex architectures, where custom behavior or\n",
        "layer configurations are needed, it is generally better to use the class-based method.\n",
        "Personally, I tend to avoid the `nn.Sequential` method, even though it can be quite convenient for simpler cases.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "VcOVkh2U3hNg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412
        },
        "outputId": "1cf300b7-cd38-4a13-e520-c9d30db06baa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, loss = 38.28511810\n",
            "Epoch 11, loss = 0.15921214\n",
            "Epoch 21, loss = 0.10976579\n",
            "Epoch 31, loss = 0.07623108\n",
            "Epoch 41, loss = 0.05298983\n",
            "Epoch 51, loss = 0.03696627\n",
            "Epoch 61, loss = 0.02596537\n",
            "Epoch 71, loss = 0.01843089\n",
            "Epoch 81, loss = 0.01327113\n",
            "Epoch 91, loss = 0.00972800\n",
            "Epoch 101, loss = 0.00728033\n",
            "Epoch 111, loss = 0.00557324\n",
            "Epoch 121, loss = 0.00436669\n",
            "Epoch 131, loss = 0.00349947\n",
            "Epoch 141, loss = 0.00286336\n",
            "Epoch 151, loss = 0.00238617\n",
            "Epoch 161, loss = 0.00201960\n",
            "Epoch 171, loss = 0.00173119\n",
            "Epoch 181, loss = 0.00149910\n",
            "Epoch 191, loss = 0.00130849\n",
            "Prediction after training: model([5]) = tensor([[10.0530, 14.9428]], grad_fn=<AddmmBackward0>)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nWhile `nn.Sequential` allows us to construct simple neural networks in a neat and compact way, \\nit is limited in terms of flexibility. For more complex architectures, where custom behavior or \\nlayer configurations are needed, it is generally better to use the class-based method. \\nPersonally, I tend to avoid the `nn.Sequential` method, even though it can be quite convenient for simpler cases.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Convention of the Input/Output Tensor Shapes (IMPORTANT)**\n",
        "\n",
        "In machine learning, it’s essential to understand how data is typically formatted when passed into models as tensors. As we have demonstrated with explicit matrix multiplications (`matmul`), when a single data point passes through the network, we expect to see tensor operations with the following shapes:\n",
        "\n",
        "$$(1, 1)  @  (1, 3)  @  (3, 3)  @  (3, 1),$$\n",
        "\n",
        "where the hidden neurons align in columns between the matrix multiplications.\n",
        "\n",
        "Thanks to the parallel computation capabilities of GPUs, we can process multiple data points simultaneously. This generalizes the former expression to:\n",
        "\n",
        "$$(\\text{Batch Size}, 1)  @  (1, 3)  @  (3, 3)  @  (3, 1),$$\n",
        "\n",
        "without violating the mathematical rules of matrix operations. More generally, we can represent the tensor operations as:\n",
        "\n",
        "$$\n",
        "(\\text{Batch Size}, \\text{Input Dimension})  @  (\\text{Input Dimension}, \\text{Hidden}_1 \\ \\text{Dimension})  @  \\cdots  @  (\\text{Hidden}_\\text{last} \\ \\text{Dimension}, \\text{Output Dimension}),\n",
        "$$\n",
        "\n",
        "where the final output shape becomes $(\\text{Batch Size}, \\text{Output Dimension})$.\n",
        "\n",
        "The input dimension is also commonly referred to as \"features.\" Hence:\n",
        "\n",
        "### **Input Tensor Shape**\n",
        "\n",
        "$$\n",
        "\\text{Input Tensor Shape:} \\quad (\\text{Batch Size}, \\text{ Number of Features})\n",
        "$$\n",
        "\n",
        "- **Batch Size (N)**: This represents the number of samples processed in parallel. Instead of feeding a single data point into the model, we usually process a batch of data points. This helps optimize training by making better use of modern hardware, such as GPUs.\n",
        "- **Number of Features (M)**: Each sample consists of multiple features. For instance, in a dataset where each sample represents an image, the features could be pixel values. In tabular data, the features might represent different measurements for each sample.\n",
        "\n",
        "Mathematically, this input tensor can be thought of as a matrix of shape $ N \\times F $, where:\n",
        "\n",
        "### **Visual Representation of the Input Tensor Shape**\n",
        "\n",
        "$$\n",
        "\\begin{pmatrix}\n",
        "\\text{feature}_1^{(1)} & \\text{feature}_2^{(1)} & \\cdots & \\text{feature}_n^{(1)} \\\\\n",
        "\\text{feature}_1^{(2)} & \\text{feature}_2^{(2)} & \\cdots & \\text{feature}_n^{(2)} \\\\\n",
        "\\text{feature}_1^{(3)} & \\text{feature}_2^{(3)} & \\cdots & \\text{feature}_n^{(3)} \\\\\n",
        "\\vdots                 & \\vdots                 & \\ddots & \\vdots \\\\\n",
        "\\text{feature}_1^{(m)} & \\text{feature}_2^{(m)} & \\cdots & \\text{feature}_n^{(m)}\n",
        "\\end{pmatrix}\n",
        "\\begin{array}{l}\n",
        "\\left. \\begin{array}{c} \\\\ \\\\ \\\\ \\\\ \\\\ \\end{array} \\right\\} \\text{Features }N; \\text{Batches }M\n",
        "\\end{array}\n",
        "$$\n",
        "\n",
        "Note that when dealing with data with higher dimensions (such as 32x32 2D-image data) we often flatten these images into a 1D vector with $32 \\times 32 = 1024$ components. This can be easily achieved using `torch.view` to reshape the original data tensor."
      ],
      "metadata": {
        "id": "qcbn7eXlX9wv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lAsGFJEeYdrk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we are equipped with the foundational skills to code a simple neural network using PyTorch’s torch.nn class. Every advanced neural network, no matter how complex, begins with these fundamental building blocks provided by the torch.nn module (or sometimes even from scratch). As the reader may have observed, our current network is simply a series of linear transformations. In the next section, we will explore how to introduce non-linearity into the model by adding *activation functions*, which are crucial for capturing more complex patterns"
      ],
      "metadata": {
        "id": "_5Mmy_qLkpUy"
      }
    }
  ]
}
