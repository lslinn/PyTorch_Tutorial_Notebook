{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we will learn how to use the `.backward()` function for gradient descent and see how PyTorch's optimizers help update the model's parameters.\n",
        "\n",
        "The process of updating a model's parameters involves three main steps:\n",
        "- **Forward pass**: Constructing the computational graph from the input to the loss.\n",
        "- **Backward pass**: Computing the gradients using the Chain Rule, typically through the \"vector-Jacobian product\" method.\n",
        "- **Model optimization**: Updating model parameters using a PyTorch optimizer.\n",
        "\n",
        "Each of these steps can be done with help from PyTorch.\n",
        "\n",
        "*Reference video: https://www.youtube.com/watch?v=3Kb0QS6z7WA&list=PLqnslRFeH2UrcDBWF5mfPGpqQDSta6VK4&index=4 (for backward pass)*\n",
        "\n",
        "*https://www.youtube.com/watch?v=E-I2DNVzQLg&list=PLqnslRFeH2UrcDBWF5mfPGpqQDSta6VK4&index=5 (for parameter updates)*\n",
        "\n",
        "*https://www.youtube.com/watch?v=VVDHU_TWwUg&list=PLqnslRFeH2UrcDBWF5mfPGpqQDSta6VK4&index=7 (for PyTorch built-in optimizer)*"
      ],
      "metadata": {
        "id": "kwDFSO1l1hdc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "id": "ZejmrE_s1ggX",
        "outputId": "b9757125-9454-40b5-f39b-0f3a5cd37a20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1., grad_fn=<PowBackward0>)\n",
            "tensor(-2.)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nExercise: Try to check by hand if ∂(loss) / ∂(w) = -2.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "\"\"\"\n",
        "Let us consider the following computational graph:\n",
        "\n",
        "x --|\n",
        "    | --> mul (*) --> ŷ --|\n",
        "w --|                     |--> sub (-) --> s --> square (^2) --> loss\n",
        "                      y --|\n",
        "\n",
        "where `x` is the input, and the loss is calculated as:\n",
        "\n",
        "loss = [(x * w) - y]^2, where (x * w) = ŷ and [(x * w) - y] = s.\n",
        "The goal is simple:\n",
        "Minimizing the loss <--> finding the value of `w` that makes `x * w` as close to the target `y` as possible.\n",
        "\n",
        "Let us assume the initial conditions: x = 1, w = 1, and y = 2. Then the setup is:\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "\n",
        "x = torch.tensor(1.0)\n",
        "y = torch.tensor(2.0)\n",
        "\n",
        "w = torch.tensor(1.0, requires_grad=True)  # requires_grad=True since we are aiming to update `w`\n",
        "\n",
        "# forward pass\n",
        "y_hat = w * x  # ŷ = w * x\n",
        "loss = (y_hat - y)**2  # loss = (ŷ - y)^2\n",
        "\n",
        "\"\"\"\n",
        "Exercise: Do you think `y_hat` and `loss` now require gradients?\n",
        "\"\"\"\n",
        "print(loss)\n",
        "\n",
        "# backward pass\n",
        "loss.backward()\n",
        "print(w.grad)  # output: tensor(-2.)\n",
        "\n",
        "\"\"\"\n",
        "Exercise: Try to check by hand if ∂(loss) / ∂(w) = -2.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Before we use PyTorch to update a model, let's manually explore how we would tackle a simple regression problem,\n",
        "where we aim to find a function f(x) = 2 * x, but the weight \"2\" is unknown initially.\n",
        "\n",
        "Steps of the regression problem include:\n",
        "1. Starting from some known dataset.\n",
        "2. Initializing the parameter for a linear function f(x) = w * x.\n",
        "3. Using this f(x) = w * x to make predictions (forward pass).\n",
        "4. Using the predictions to calculate the loss.\n",
        "5. Using the loss to compute gradients with the Chain Rule (backward pass).\n",
        "6. Iteratively updating w using the gradients (gradient descent).\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# 1: Known dataset used for training the linear model\n",
        "X = np.array([1, 2, 3, 4], dtype=np.float32)\n",
        "Y = np.array([2, 4, 6, 8], dtype=np.float32)\n",
        "# This suggests Y = 2 * X\n",
        "\n",
        "# 2: Initialize the weight w (which we don't know initially)\n",
        "w = 0.0  # Initial guess for w\n",
        "\n",
        "# 3: Model prediction (define forward pass)\n",
        "def forward(x):\n",
        "    return w * x\n",
        "\n",
        "# 4: define the loss (Mean Squared Error)\n",
        "def loss(y, y_predicted):\n",
        "    return ((y_predicted - y) ** 2).mean()\n",
        "\n",
        "# 5: Gradient calculation\n",
        "# MSE = 1/N * sum((w * x - y)^2)\n",
        "# dJ/dw = 1/N * 2 * x * (w * x - y), where J is the loss\n",
        "\n",
        "def gradient(x, y, y_predicted):\n",
        "    return np.dot(2 * x, y_predicted - y).mean()\n",
        "\n",
        "print(f'Prediction before training: f(5) = {forward(5):.3f}')\n",
        "\n",
        "# 6: Main training loop\n",
        "learning_rate = 0.01\n",
        "n_iterations = 10\n",
        "\n",
        "for epoch in range(n_iterations):\n",
        "    # Prediction (forward pass)\n",
        "    y_pred = forward(X)\n",
        "\n",
        "    \"\"\"\n",
        "    Note: We are passing a vector of data (X) in each forward pass, which matches with Y.\n",
        "    You can think of this as passing a `batch` of data. This is why we use `.mean()`\n",
        "    in the loss and gradient functions to average over the batch.\n",
        "    \"\"\"\n",
        "\n",
        "    # Calculating loss\n",
        "    l = loss(Y, y_pred)\n",
        "\n",
        "    # Calculating gradients\n",
        "    dw = gradient(X, Y, y_pred)\n",
        "\n",
        "    # Update weights (Gradient \"Descent\")\n",
        "    w -= learning_rate * dw\n",
        "\n",
        "    # Print training information\n",
        "    if epoch % 1 == 0:\n",
        "        print(f'Epoch {epoch + 1}: w = {w:.3f}, loss = {l:.8f}')\n",
        "\n",
        "print(f'Prediction after training: f(5) = {forward(5):.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EsZQf4am6R2m",
        "outputId": "42763ccb-f40d-4ae5-ff6a-eaf5420fc0e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction before training: f(5) = 0.000\n",
            "Epoch 1: w = 1.200, loss = 30.00000000\n",
            "Epoch 2: w = 1.680, loss = 4.79999924\n",
            "Epoch 3: w = 1.872, loss = 0.76800019\n",
            "Epoch 4: w = 1.949, loss = 0.12288000\n",
            "Epoch 5: w = 1.980, loss = 0.01966083\n",
            "Epoch 6: w = 1.992, loss = 0.00314574\n",
            "Epoch 7: w = 1.997, loss = 0.00050331\n",
            "Epoch 8: w = 1.999, loss = 0.00008053\n",
            "Epoch 9: w = 1.999, loss = 0.00001288\n",
            "Epoch 10: w = 2.000, loss = 0.00000206\n",
            "Prediction after training: f(5) = 9.999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Now, let's move on and see how the PyTorch version of this process will look.\n",
        "We'll use PyTorch's built-in functionalities to handle gradient computation and parameter updates more efficiently.\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "\n",
        "# Convert the dataset into PyTorch tensors for Autograd\n",
        "X = torch.tensor(X, dtype=torch.float32)\n",
        "Y = torch.tensor(Y, dtype=torch.float32)\n",
        "\n",
        "# Create the weight parameter with requires_grad=True for grad tracking\n",
        "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
        "\n",
        "# Forward pass function (remains the same; but we'll use PyTorch tensors for input)\n",
        "def forward(x):\n",
        "    return w * x\n",
        "\n",
        "# Loss function (remains the same; PyTorch tensors input)\n",
        "def loss(y, y_predicted):\n",
        "    return ((y_predicted - y) ** 2).mean()\n",
        "\n",
        "print(f'Prediction before training: f(5) = {forward(5):.3f}')\n",
        "\n",
        "# Main training loop\n",
        "learning_rate = 0.01\n",
        "n_iterations = 20\n",
        "\n",
        "for epoch in range(n_iterations):\n",
        "    # Forward pass\n",
        "    y_pred = forward(X)\n",
        "\n",
        "    # Calculate loss\n",
        "    l = loss(Y, y_pred)\n",
        "\n",
        "    # Backward pass to compute gradients\n",
        "    l.backward()  # PyTorch handles the gradient computation AUTOMATICALLY!\n",
        "                  # We don't need to calculate the gradients using Chain Rule by ourself!\n",
        "\n",
        "    # Update weights\n",
        "    with torch.no_grad():\n",
        "        w -= learning_rate * w.grad\n",
        "    \"\"\"\n",
        "    Note: We use `torch.no_grad()` to ensure that the weight update does not interfere with the computational graph.\n",
        "    This prevents PyTorch from tracking gradients during the update step.\n",
        "    \"\"\"\n",
        "\n",
        "    # Zero the gradients after updating\n",
        "    w.grad.zero_()\n",
        "    \"\"\"\n",
        "    IMPORTANT: Gradients accumulate with each call to `.backward()`, so we must reset (zero) them\n",
        "    after each weight update and before the next backward pass.\n",
        "    \"\"\"\n",
        "\n",
        "    # Print training info\n",
        "    if epoch % 2 == 0:\n",
        "        print(f'Epoch {epoch + 1}: w = {w:.3f}, loss = {l:.8f}')\n",
        "\n",
        "print(f'Prediction after training: f(5) = {forward(5):.3f}')\n",
        "\n",
        "\"\"\"\n",
        "Note: The result of f(5) might not match exactly with the manual NumPy model.\n",
        "The differences are likely due to floating-point precision in training,\n",
        "but the gradients computed by backpropagation in PyTorch are exact, just like the manual calculation.\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "vZeGEImfCxuu",
        "outputId": "e5f209d1-cdbc-414e-f75f-d90f2c7e416c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction before training: f(5) = 0.000\n",
            "Epoch 1: w = 0.300, loss = 30.00000000\n",
            "Epoch 3: w = 0.772, loss = 15.66018772\n",
            "Epoch 5: w = 1.113, loss = 8.17471695\n",
            "Epoch 7: w = 1.359, loss = 4.26725292\n",
            "Epoch 9: w = 1.537, loss = 2.22753215\n",
            "Epoch 11: w = 1.665, loss = 1.16278565\n",
            "Epoch 13: w = 1.758, loss = 0.60698116\n",
            "Epoch 15: w = 1.825, loss = 0.31684780\n",
            "Epoch 17: w = 1.874, loss = 0.16539653\n",
            "Epoch 19: w = 1.909, loss = 0.08633806\n",
            "Prediction after training: f(5) = 9.612\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nNote: The result of f(5) might not match exactly with the manual NumPy model.\\nThe differences are likely due to floating-point precision in training,\\nbut the gradients computed by backpropagation in PyTorch are exact, just like the manual calculation.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we understand how PyTorch's autograd simplifies the training process. Up to this point, the key takeaway is that autograd frees us from manually tracking and calculating gradients.\n",
        "\n",
        "Next, we will go further by learning how PyTorch's optimizers can help with parameter updates. In the current simple model, we used a static learning rate. In more complex scenarios, advanced optimization algorithms like `Adam`, which dynamically adjust the learning rate, are often preferred.\n",
        "\n",
        "Putting aside the benefits of advanced algorithms, even basic methods like Stochastic Gradient Descent (SGD) are simplified with PyTorch, making our code more efficient and concise."
      ],
      "metadata": {
        "id": "MTK0A1stfh2p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's do the same example but with a PyTorch optimizer\n",
        "\n",
        "# Creating dataset in PyTorch tensors\n",
        "X = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n",
        "Y = torch.tensor([2, 4, 6, 8], dtype=torch.float32)\n",
        "\n",
        "# Create the weight parameter with requires_grad=True for gradient tracking\n",
        "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
        "\n",
        "# The same forward pass function\n",
        "def forward(x):\n",
        "    return w * x\n",
        "\n",
        "# The same mean squared error (MSE) loss function\n",
        "def loss(y, y_predicted):\n",
        "    return ((y_predicted - y) ** 2).mean()\n",
        "\n",
        "print(f'Prediction before training: f(5) = {forward(5):.3f}')\n",
        "\n",
        "# Main training loop settings\n",
        "learning_rate = 0.01\n",
        "n_iterations = 100\n",
        "\n",
        "\"\"\"\n",
        "Now, let's set up a PyTorch optimizer before starting the training loop\n",
        "to update our model using stochastic gradient descent (SGD).\n",
        "\n",
        "PyTorch's optimizers are part of the `torch.optim` module.\n",
        "You can explore many different optimizers in the official documentation:\n",
        "https://pytorch.org/docs/stable/optim.html\n",
        "\n",
        "Let's start with a simple one: `torch.optim.SGD`.\n",
        "The optimizer is instantiated by passing the parameters we want to update (in a list)\n",
        "and the learning rate (lr).\n",
        "\n",
        "Note: The learning rate is a fundamental hyperparameter that you should tune\n",
        "first if the model's performance is not satisfactory.\n",
        "\"\"\"\n",
        "optimizer = torch.optim.SGD([w], lr=learning_rate)\n",
        "\n",
        "for epoch in range(n_iterations):\n",
        "    # Forward pass\n",
        "    y_pred = forward(X)\n",
        "\n",
        "    # Calculate loss\n",
        "    l = loss(Y, y_pred)\n",
        "\n",
        "    # Backward pass to compute gradients\n",
        "    l.backward()\n",
        "\n",
        "    # Update weights. The SGD optimizer updates the weights using the `.grad` attribute on the parameters\n",
        "    # which was populated by the `.backward()` call.\n",
        "    optimizer.step()\n",
        "\n",
        "    # with torch optimizer, the way to zero gradients is simple:\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Print training info\n",
        "    if epoch % 10 == 0:\n",
        "        print(f'Epoch {epoch + 1}: w = {w:.3f}, loss = {l:.8f}')\n",
        "\n",
        "print(f'Prediction after training: f(5) = {forward(5):.3f}')\n",
        "\n",
        "## We see how clean and efficient our code becomes with the help of PyTorch optimizers."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JT4UQEz2vWKW",
        "outputId": "66523420-796e-435d-ffcc-be6feb5e54a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction before training: f(5) = 0.000\n",
            "Epoch 1: w = 0.300, loss = 30.00000000\n",
            "Epoch 11: w = 1.665, loss = 1.16278565\n",
            "Epoch 21: w = 1.934, loss = 0.04506890\n",
            "Epoch 31: w = 1.987, loss = 0.00174685\n",
            "Epoch 41: w = 1.997, loss = 0.00006770\n",
            "Epoch 51: w = 1.999, loss = 0.00000262\n",
            "Epoch 61: w = 2.000, loss = 0.00000010\n",
            "Epoch 71: w = 2.000, loss = 0.00000000\n",
            "Epoch 81: w = 2.000, loss = 0.00000000\n",
            "Epoch 91: w = 2.000, loss = 0.00000000\n",
            "Prediction after training: f(5) = 10.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### About the choice of optimizers:\n",
        "\n",
        "Very nice YouTube reference: [ML 2021 (English version)] Lecture 6: What to do when optimization fails? (Learning rate):\n",
        "https://www.youtube.com/watch?v=8yf-tU7zm7w\n",
        "(describing how learning works in different optimization strategies)\n",
        "\n",
        "When training machine learning models, choosing the right optimizer is important for performance. Different optimizers can lead to different convergence speeds and results. Here are a few commonly used optimizers in PyTorch, starting from the more basic to the more advanced:\n",
        "\n",
        "1. **Stochastic Gradient Descent (SGD)**:\n",
        "   - This is the most basic and foundational optimizer. It updates parameters based on the gradients, typically with a fixed learning rate.\n",
        "   - While effective, SGD can struggle with more complex models due to its constant learning rate, but it can still perform well with careful tuning and the addition of momentum.\n",
        "   - `torch.optim.SGD`\n",
        "\n",
        "2. **Momentum-based SGD**:\n",
        "   - A variant of SGD that introduces momentum, which helps the optimizer converge faster by building up speed in directions with consistent gradients.\n",
        "   - Momentum helps overcome small local minima or flat regions in the loss function, which plain SGD may struggle with.\n",
        "   - `torch.optim.SGD` with the `momentum` parameter.\n",
        "\n",
        "3. **RMSprop**:\n",
        "   - RMSprop divides the learning rate by an exponentially decaying average of squared gradients, which helps mitigate large gradient updates.\n",
        "   - It is often used in recurrent neural networks (RNNs) and time-series models due to its ability to adapt to non-stationary data.\n",
        "   - Example: `torch.optim.RMSprop`\n",
        "\n",
        "4. **AdaGrad**:\n",
        "   - AdaGrad adapts the learning rate for each parameter based on the magnitude of its gradient, making it particularly suitable for sparse data.\n",
        "   - However, it can cause the learning rate to become too small over time, which may slow down learning.\n",
        "   - `torch.optim.Adagrad`\n",
        "\n",
        "5. **Adam (Adaptive Moment Estimation)**:\n",
        "   - Adam is one of the most commonly used optimizers in modern deep learning. It combines the benefits of AdaGrad (adaptive learning rates) and RMSprop (exponentially decaying average of squared gradients) while also incorporating momentum-like behavior.\n",
        "   - A common parameter used with Adam is `weight_decay`, which applies L2 regularization. This helps prevent overfitting by penalizing large weights and is widely used in practice.\n",
        "   - These make Adam particularly effective for larger models, deep neural networks, and noisy data, which is why it’s often the default choice in modern machine learning.\n",
        "   - `torch.optim.Adam`\n",
        "\n",
        "### Which optimizer to choose?\n",
        "- **SGD** is a good starting point for simple models and experimentation.\n",
        "- **Adam** is the most widely used optimizer for modern deep learning models due to its adaptability and robust performance on complex problems.\n",
        "- Regardless of the optimizer you choose, tuning hyperparameters like the learning rate is essential for achieving good results.\n",
        "\n",
        "###P.S. If you're unsure what to use, **go with Adam**."
      ],
      "metadata": {
        "id": "lpf5NDS196GT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have learned how to use PyTorch for updating a model. However, a famous structure — the neural network — has yet to appear. In the coming tutorial, we will explore more complex examples and start learning how to build a neural network — straight from scratch."
      ],
      "metadata": {
        "id": "qYGJzhAgvVau"
      }
    }
  ]
}
