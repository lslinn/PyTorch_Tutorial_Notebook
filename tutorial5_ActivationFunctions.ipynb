{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# In this tutorial, we will learn about how to incorporate activation functions for adding nonlinearity to our Neural Network.\n",
        "\n",
        "* To learn the role of activation function in NN, please see the following reference video: https://www.youtube.com/watch?v=3t9lZM7SS7k&list=PLqnslRFeH2UrcDBWF5mfPGpqQDSta6VK4&index=12 or https://www.youtube.com/watch?v=s-V7gKrsels\n",
        "\n",
        "* To visualize and play with different network structures and activations, go to -- A Neural Network Playground:\n",
        "https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://playground.tensorflow.org/&ved=2ahUKEwint9HS1-yIAxWpj68BHYMgEpIQFnoECBcQAQ&usg=AOvVaw2vOMcWmUkU5HmbODfylijL"
      ],
      "metadata": {
        "id": "-HOrhPQNsDFC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Let us start with a similar problem from the previous section\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "# Creating dataset in PyTorch tensors\n",
        "X = torch.tensor([[1], [2], [3], [4]], dtype=torch.float32)  # shape (4, 1)\n",
        "Y = torch.tensor([[2, 1], [4, 4], [6, 9], [8, 16]], dtype=torch.float32)  # shape (4, 2)\n",
        "\"\"\"\n",
        "The difference is, the first position of the output remains as f(x) = 2 * x,\n",
        "yet for the second position of the output, we are aiming at f(x) = x^2\n",
        "\"\"\"\n",
        "\n",
        "# Building NN same as the previous section\n",
        "class NN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(NN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)  # first hidden\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)  # second hidden\n",
        "        self.fc_out = nn.Linear(hidden_size, output_size) # output layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.fc_out(x)\n",
        "        return x\n",
        "\n",
        "# Instanciating NN, but with hidden dimension = 10 now expecting the task may be more difficult\n",
        "model = NN(input_size=X.shape[1], hidden_size=3, output_size=Y.shape[1])\n",
        "\n",
        "# def MSE loss\n",
        "def loss(y, y_predicted):\n",
        "    return ((y_predicted - y) ** 2).mean()\n",
        "\n",
        "# Initial prediction\n",
        "print(f'Prediction before training: model([5]) = {model.forward(torch.tensor([[5]], dtype=torch.float32))}')\n",
        "\n",
        "# Main training loop settings\n",
        "learning_rate = 0.01\n",
        "n_iterations = 10000\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for epoch in range(n_iterations):\n",
        "    # Forward pass\n",
        "    y_pred = model.forward(X)\n",
        "\n",
        "    # Calculate loss\n",
        "    l = loss(Y, y_pred)\n",
        "\n",
        "    # Zero gradients:: Putting zero_grad before backward is also fine!\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Backward pass to compute gradients\n",
        "    l.backward()\n",
        "\n",
        "    # Optimizer step\n",
        "    optimizer.step()\n",
        "\n",
        "    # Print training info\n",
        "    if epoch % 1000 == 0:\n",
        "        print(f'Epoch {epoch + 1}, loss = {l:.8f}')\n",
        "\n",
        "# Prediction after training\n",
        "print(f'Prediction after training: model([5]) = {model.forward(torch.tensor([[5]], dtype=torch.float32))}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHGg7kFwsCwy",
        "outputId": "86497946-d0b5-4a82-9455-428784e4702c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction before training: model([5]) = tensor([[ 0.4682, -0.0427]], grad_fn=<AddmmBackward0>)\n",
            "Epoch 1, loss = 58.43006897\n",
            "Epoch 1001, loss = 0.49999991\n",
            "Epoch 2001, loss = 0.49999961\n",
            "Epoch 3001, loss = 0.49999961\n",
            "Epoch 4001, loss = 0.49999961\n",
            "Epoch 5001, loss = 0.49999961\n",
            "Epoch 6001, loss = 0.49999961\n",
            "Epoch 7001, loss = 0.49999961\n",
            "Epoch 8001, loss = 0.49999961\n",
            "Epoch 9001, loss = 0.49999961\n",
            "Prediction after training: model([5]) = tensor([[10.0000, 20.0000]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "How about the intermediate values?\n",
        "\"\"\"\n",
        "for i in range(5):\n",
        "    print(f'Prediction after training: model([{i+1}]) = {model.forward(torch.tensor([[i+1]], dtype=torch.float32))}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4SMMX0O87nCg",
        "outputId": "98721e45-7f54-4dcf-cd6e-69df22d3b673"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction after training: model([1]) = tensor([[2.0000e+00, 6.2585e-07]], grad_fn=<AddmmBackward0>)\n",
            "Prediction after training: model([2]) = tensor([[4.0000, 5.0000]], grad_fn=<AddmmBackward0>)\n",
            "Prediction after training: model([3]) = tensor([[ 6.0000, 10.0000]], grad_fn=<AddmmBackward0>)\n",
            "Prediction after training: model([4]) = tensor([[ 8.0000, 15.0000]], grad_fn=<AddmmBackward0>)\n",
            "Prediction after training: model([5]) = tensor([[10.0000, 20.0000]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Here,\n",
        "we find that the first output can get close as expected. However, the second struggles, and no matter how much we adjust the `learning_rate`, `n_iterations`, or optimizers, the model fails to match the desired quadratic relationship.\n",
        "\n",
        "This is not surprising because our neural network is built using only “linear layers.” Linear layers alone are not capable of modeling non-linear relationships like  $f(x) = x^2$.\n",
        "\n",
        "To solve this, we need to introduce nonlinearity into the model. A common way to do this is by applying a non-linear activation function after each linear layer, as shown below:\n",
        "\n",
        "$$a_i^{(l)} = f\\left(\\sum_{j=1}^{n_{l-1}} w_{ij}^{(l)} a_j^{(l-1)} + b_i^{(l)}\\right)$$\n",
        "\n",
        "where:\n",
        "- $a_i^{(l)}$ represents the activation/output of the $i$-th neuron in layer $l$,\n",
        "- $w_{ij}^{(l)}$ is the weight connecting the $j$-th neuron in layer $(l-1)$ to the $i$-th neuron in layer $l$,\n",
        "- $b_i^{(l)}$ is the bias term for the $i$-th neuron in layer $l$,\n",
        "- $a_j^{(l-1)}$ is the activation/output of the $j$-th neuron in the previous layer $(l-1)$\n",
        "- and $f(\\cdot)$ is a non-linear activation functions like ReLU, Sigmoid, or Tanh, which introduces nonlinearity between layers.\n",
        "\n",
        "By wrapping the linear combinations of weights, inputs, and biases with a non-linear activation function $f$, the neural network gains the ability to learn more complex, non-linear patterns, like the quadratic relationship in our example.\n",
        "\n",
        "There are many activation functions at work in neural networks. Some of the most commonly used activation functions—like Sigmoid, Tanh, ReLU, Leaky ReLU, and Softmax—are introduced in the reference video: *https://www.youtube.com/watch?v=3t9lZM7SS7k&list=PLqnslRFeH2UrcDBWF5mfPGpqQDSta6VK4&index=12*.\n",
        "\n",
        "Choosing the right activation function is an important part of neural network design and can vary from model to model. ReLU is likely the most widely used due to its simplicity and effectiveness in many deep learning tasks."
      ],
      "metadata": {
        "id": "nNzgepZ0xJ8t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09_WHXRusBlj",
        "outputId": "bf9a508f-179d-4887-b163-8d869e45066c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction before training: model([5]) = tensor([[0.2827, 1.0795]], grad_fn=<AddmmBackward0>)\n",
            "Epoch 1, loss = 52.44388199\n",
            "Epoch 1001, loss = 0.18804431\n",
            "Epoch 2001, loss = 0.03756940\n",
            "Epoch 3001, loss = 0.00843241\n",
            "Epoch 4001, loss = 0.00213237\n",
            "Epoch 5001, loss = 0.00063580\n",
            "Epoch 6001, loss = 0.00018977\n",
            "Epoch 7001, loss = 0.00005521\n",
            "Epoch 8001, loss = 0.00001542\n",
            "Epoch 9001, loss = 0.00000457\n",
            "Prediction after training: model([5]) = tensor([[10.0462, 22.8948]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Now, let us incorporate the activation function into our model to see if it improves the prediction.\n",
        "We will use ReLU here. To do so, we can either use the module `nn.ReLU()` or the function `torch.relu()`.\n",
        "Of course, you can also define the ReLU function by yourself: ReLU(x) = max(0, x)\n",
        "\"\"\"\n",
        "\n",
        "# Creating dataset in PyTorch tensors\n",
        "X = torch.tensor([[1], [2], [3], [4]], dtype=torch.float32)  # shape (4, 1)\n",
        "Y = torch.tensor([[2, 1], [4, 4], [6, 9], [8, 16]], dtype=torch.float32)  # shape (4, 2)\n",
        "\n",
        "# NN with activation function (ReLU)\n",
        "class NN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(NN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)  # first hidden\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)  # second hidden\n",
        "        self.fc_out = nn.Linear(hidden_size, output_size) # output layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc_out(x)\n",
        "        return x\n",
        "\n",
        "# Instanciating NN, with hidden layer dimension 10\n",
        "model = NN(input_size=X.shape[1], hidden_size=10, output_size=Y.shape[1])\n",
        "\n",
        "# def MSE loss\n",
        "def loss(y, y_predicted):\n",
        "    return ((y_predicted - y) ** 2).mean()\n",
        "\n",
        "# Initial prediction\n",
        "print(f'Prediction before training: model([5]) = {model.forward(torch.tensor([[5]], dtype=torch.float32))}')\n",
        "\n",
        "# Main training loop settings\n",
        "learning_rate = 0.01\n",
        "n_iterations = 10000\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for epoch in range(n_iterations):\n",
        "    # Forward pass\n",
        "    y_pred = model.forward(X)\n",
        "\n",
        "    # Calculate loss\n",
        "    l = loss(Y, y_pred)\n",
        "\n",
        "    # Zero gradients:: Putting zero_grad before backward is also fine!\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Backward pass to compute gradients\n",
        "    l.backward()\n",
        "\n",
        "    # Optimizer step\n",
        "    optimizer.step()\n",
        "\n",
        "    # Print training info\n",
        "    if epoch % 1000 == 0:\n",
        "        print(f'Epoch {epoch + 1}, loss = {l:.8f}')\n",
        "\n",
        "# Prediction after training\n",
        "print(f'Prediction after training: model([5]) = {model.forward(torch.tensor([[5]], dtype=torch.float32))}')\n",
        "\n",
        "## DO YOU EXPECT A [10, 25]?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "You may find that the result is still not perfect.\n",
        "However, if you check the predictions within the range of the training data, you’ll notice:\n",
        "\"\"\"\n",
        "for i in range(5):\n",
        "    print(f'Prediction after training: model([{i+1}]) = {model.forward(torch.tensor([[i+1]], dtype=torch.float32))}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0xcWk8O5tE3",
        "outputId": "07d30aa1-0546-42a8-a181-accf0a1a1d26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction after training: model([1]) = tensor([[2.0001, 1.0002]], grad_fn=<AddmmBackward0>)\n",
            "Prediction after training: model([2]) = tensor([[4.0004, 4.0007]], grad_fn=<AddmmBackward0>)\n",
            "Prediction after training: model([3]) = tensor([[6.0006, 9.0022]], grad_fn=<AddmmBackward0>)\n",
            "Prediction after training: model([4]) = tensor([[ 8.0008, 16.0031]], grad_fn=<AddmmBackward0>)\n",
            "Prediction after training: model([5]) = tensor([[10.0462, 22.8948]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "You’ll find that the model outputs very precise results **within** the range of\n",
        "the data it was trained on. This leads us to two important conclusions:\n",
        "\n",
        "1. **Compared to the version without an activation function, our model can now\n",
        "learn non-linear relationships**. This is clear from how well the model predicts\n",
        "both linear (2 * x) and non-linear (x^2) patterns with input [[1], [2], [3], [4]].\n",
        "\n",
        "2. **However, DON’T expect a simple neural network to generalize well beyond the\n",
        "training data range!** The model performs well within the range it was trained on,\n",
        "but very often, a simple neural network can overfit to the training data, leading\n",
        "to poor generalization outside of that range.\n",
        "\n",
        "--- The information we don’t have, we wouldn’t have.\n",
        "Yet, the interest lies in the information that hides in complex patterns,\n",
        "which human modelers may not initially notice. ---\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "EXERCISE,\n",
        "1. how would you apply activation functions to our \"from scratch\" model?\n",
        "2. how about playing with some other activation functions from:\n",
        "   https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity\n",
        "   (search: activations)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "qmGmq4QK5t6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Additional Information:\n",
        "\n",
        "Deep neural network? Shallow neural network? Which network structure is the best?\n",
        "https://www.youtube.com/watch?v=oJNHXPs0XDk\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "M-_O4uXV_GQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we are equipped with most of the fundamental knowledge for building a neural network using PyTorch. Advanced algorithms largely focus on the architecture of neural networks (e.g. see the Additional Information) and strategies for updating them in different tasks. In the next tutorial, we will apply what we have learned to a basic exercise: solving a simple image classification problem."
      ],
      "metadata": {
        "id": "pW3z8t4EvTrv"
      }
    }
  ]
}
