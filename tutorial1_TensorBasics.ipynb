{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "We should start with learning how to create and manage some basic functions related to tensors in PyTorch.\n",
        "\n",
        "\n",
        "\n",
        "*Installation of PyTorch please refer to GET STARTED: https://pytorch.org/get-started/locally/*\n",
        "\n",
        "*Refrerece video: https://www.youtube.com/watch?v=exaWOE8jvy8&list=PLqnslRFeH2UrcDBWF5mfPGpqQDSta6VK4&index=4*\n",
        "\n",
        "*(This YouTube PyTorch tutorial is recommended)*"
      ],
      "metadata": {
        "id": "7bL8qrBlzaHE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Cpd6Bk8y66I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3fe0418-7b8c-40ce-8191-b334c49317c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor of shape (3): tensor([0., 0., 0.])\n",
            "tensor of shape (3,2): tensor([[-7.6959e-29,  4.4196e-41],\n",
            "        [-2.2742e+26,  3.1040e-41],\n",
            "        [ 4.4842e-44,  0.0000e+00]])\n",
            "the data type of x is now: torch.float32\n",
            "the shape of x is torch.Size([3, 2])\n",
            "example of rand: tensor([[[[0.4720, 0.8672, 0.0880],\n",
            "          [0.3601, 0.5712, 0.7570]],\n",
            "\n",
            "         [[0.9826, 0.8467, 0.9302],\n",
            "          [0.7855, 0.7775, 0.1663]]],\n",
            "\n",
            "\n",
            "        [[[0.6910, 0.8895, 0.9056],\n",
            "          [0.3281, 0.3815, 0.5234]],\n",
            "\n",
            "         [[0.7598, 0.5422, 0.2629],\n",
            "          [0.7194, 0.3346, 0.4623]]]])\n",
            "example with ones: tensor([[[1., 1.],\n",
            "         [1., 1.]],\n",
            "\n",
            "        [[1., 1.],\n",
            "         [1., 1.]]])\n",
            "ex1:\n",
            " tensor([1, 2, 3]) with shape torch.Size([3])\n",
            "ex2:\n",
            " tensor([[1],\n",
            "        [2],\n",
            "        [3]]) with shape torch.Size([3, 1])\n",
            "ex3:\n",
            " tensor([[1, 2, 3],\n",
            "        [4, 5, 6],\n",
            "        [7, 8, 9]]) with shape torch.Size([3, 3])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "\"\"\" with torch.empty, we can easily create empty torch tensors.\"\"\"\n",
        "x = torch.empty(3)\n",
        "print(f'tensor of shape (3): {x}')\n",
        "# this print a 1D tensor with shape 3\n",
        "\n",
        "\n",
        "\"\"\" The first entry designates the shape of the tensor.\n",
        "We can also assign the data type using `dtype` as below\"\"\"\n",
        "x = torch.empty(3,2, dtype=torch.float32)\n",
        "print(f'tensor of shape (3,2): {x}')\n",
        "# This print is an empty tensor with shape (3,2)\n",
        "\n",
        "\n",
        "\"\"\" To check the data type of a torch.tensor, we can simply call `.dtype`\"\"\"\n",
        "print(f'the data type of x is now: {x.dtype}')\n",
        "\"\"\" On the other hand, we can check the shape of x with `.size()`\"\"\"\n",
        "print(f'the shape of x is {x.size()}')\n",
        "# Note that we can also call x.shape to find the same result\n",
        "\n",
        "\n",
        "\"\"\" Besides `.empty`, tensors can be initiated differently\"\"\"\n",
        "x = torch.rand(2, 2, 2, 3)\n",
        "print(f'example of rand: {x}')\n",
        "# `.rand` gives random value components\n",
        "x = torch.ones(2, 2, 2)\n",
        "print(f'example with ones: {x}')\n",
        "# `.ones` fills tensor with given shaped with ones\n",
        "# Note that we can of course incorporate the `dtype=` method too\n",
        "\n",
        "\"\"\" To create a tensor with given values, we simply do\"\"\"\n",
        "ex1 = torch.tensor([1,2,3])\n",
        "ex2 = torch.tensor([[1], [2], [3]])\n",
        "ex3 = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
        "print(f'ex1:\\n {ex1} with shape {ex1.size()}')\n",
        "print(f'ex2:\\n {ex2} with shape {ex2.size()}')\n",
        "print(f'ex3:\\n {ex3} with shape {ex3.size()}')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Now we can learn some basic operation\n",
        "\"\"\"\n",
        "x = torch.tensor([1,2,3])\n",
        "y = torch.tensor([1,2,3])\n",
        "\n",
        "## Element-ment wise addition\n",
        "z = x + y\n",
        "print(z)\n",
        "z = torch.add(x, y) # equivilent to x + y\n",
        "print(z)\n",
        "\n",
        "# we can also do\n",
        "y.add_(x)\n",
        "print(y)\n",
        "\"\"\" IMPORTANT:\n",
        "In PyTorch, any function with a trailing underscore (\"_\"), such as .add_(),\n",
        "is an in-place operation. This means it directly modifies the data\n",
        "of the tensor it is called on, rather than creating a new tensor with\n",
        "the result. Be cautious when using in-place operations, as they change\n",
        "the content of the original tensor and may interfere with the\n",
        "`computational graph` for automatic differentiation.\"\"\"\n",
        "\n",
        "# compare with the in-place version, we can do out-of-place addition\n",
        "z = y.add(x)\n",
        "\"\"\" When using add(), PyTorch allocates new memory for the result,\n",
        "making it a separate object from the original tensors.\n",
        "In such cases, the original y can still exist in the computational graph\n",
        "and can still be tracked later by PyTorch's autograd system.\n",
        "\n",
        "Topics related to the computational graph and autograd will come later.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "## substraction\n",
        "y = torch.tensor([1,2,3]) # reset y\n",
        "z = x - y\n",
        "print(z)\n",
        "z = torch.sub(x,y)\n",
        "print(z)\n",
        "# or\n",
        "y.sub_(x)\n",
        "print(y)\n",
        "\n",
        "## element wise multiplication and division are similar\n",
        "y = torch.tensor([1,2,3])\n",
        "\n",
        "z = x * y\n",
        "z = torch.mul(x, y)\n",
        "print(z)\n",
        "\n",
        "z = x / y\n",
        "z = torch.div(x, y)\n",
        "print(z)\n",
        "\n",
        "# of course, we have `.mul_()` or `.div_()` methods here too\n",
        "\"\"\"Exercise: play with in-place multiplication and division.\n",
        "\n",
        "Do you find an error? Why?\n",
        "hint: checking `.dtype`.\n",
        "\"\"\"\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "Bn_vdHaI2pj_",
        "outputId": "ed9dae11-2466-43de-ecf4-ff65e6ff7f9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([2, 4, 6])\n",
            "tensor([2, 4, 6])\n",
            "tensor([2, 4, 6])\n",
            "tensor([0, 0, 0])\n",
            "tensor([0, 0, 0])\n",
            "tensor([0, 0, 0])\n",
            "tensor([1, 4, 9])\n",
            "tensor([1., 1., 1.])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Exercise: play with in-place multiplication and division.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Slicing\n",
        "\"\"\"\n",
        "import torch\n",
        "\n",
        "x = torch.rand(5, 5)  # 5x5 tensor of random values\n",
        "print(x)\n",
        "print(x[:, 0])  # Run through all rows at column 0 (first column)\n",
        "print(x[2:, 0])  # Starting from the 3rd row onward, for column 0\n",
        "print(x[0, :3])  # Row 0, up to (but not including) the 4rd column\n",
        "\n",
        "\"\"\"\n",
        "Note: When you create a tensor with 2 dimensions, it is a 2D tensor.\n",
        "You can think of it as x[i, j], where the first index i represents the row\n",
        "and the second index j represents the column.\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "Slicing itself is not an in-place operation but a view of the original tensor.\n",
        "It shares the same memory as the original tensor.\n",
        "Following are some demonstrations of operations.\n",
        "\"\"\"\n",
        "\n",
        "### Example: Slicing creates a view\n",
        "# Original tensor\n",
        "x = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0])\n",
        "\n",
        "# Slicing the tensor (creates a view)\n",
        "y = x[1:4]\n",
        "\n",
        "# Modifying the slice (not in-place)\n",
        "y = y + 1\n",
        "\n",
        "print(\"Original x:\", x)  # x remains unchanged\n",
        "print(\"Slice y:\", y)     # y is a new tensor with the result of the operation\n",
        "\n",
        "\n",
        "### Example: In-place Modification of a Slice\n",
        "# Original tensor\n",
        "x = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0])\n",
        "\n",
        "# Slicing the tensor (creates a view)\n",
        "y = x[1:4]\n",
        "\n",
        "# In-place modification of the slice\n",
        "y.add_(1)\n",
        "\n",
        "print(\"Original x:\", x)  # x will be modified\n",
        "print(\"Slice y:\", y)     # y also reflects the in-place modification"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WimHehxR70CF",
        "outputId": "cc11433c-0de3-4e1f-d1ac-be81bbc8c0ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.8835, 0.9454, 0.3013, 0.1823, 0.0865],\n",
            "        [0.7772, 0.3688, 0.4386, 0.4313, 0.9339],\n",
            "        [0.9562, 0.8745, 0.2060, 0.2765, 0.7073],\n",
            "        [0.9373, 0.2182, 0.6257, 0.0107, 0.7238],\n",
            "        [0.8941, 0.6045, 0.2227, 0.2359, 0.9832]])\n",
            "tensor([0.8835, 0.7772, 0.9562, 0.9373, 0.8941])\n",
            "tensor([0.9562, 0.9373, 0.8941])\n",
            "tensor([0.8835, 0.9454, 0.3013])\n",
            "Original x: tensor([1., 2., 3., 4., 5.])\n",
            "Slice y: tensor([3., 4., 5.])\n",
            "Original x: tensor([1., 3., 4., 5., 5.])\n",
            "Slice y: tensor([3., 4., 5.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Reshaping a tensor with `.view`\n",
        "\"\"\"\n",
        "x = torch.rand(4,4)\n",
        "print(x)\n",
        "# We can use the view method to reshape a tensor\n",
        "# it is like \"viewing\" the components in a different way\n",
        "# remember what we have just discussed about \"the view of an object\"\n",
        "\n",
        "y = x.view(16)\n",
        "print(y)\n",
        "# we can designate what kind of shape we want for viewing this object\n",
        "# in this example, we simply ask for a 1D, 16-component vector.\n",
        "\n",
        "# we can also do\n",
        "y = x.view(8, 2)\n",
        "print(y)\n",
        "\n",
        "# The number of components should be consistent.\n",
        "# We could also let the function assign the dimensions automatically\n",
        "# by inputting -1:\n",
        "\n",
        "y = x.view(2, -1)\n",
        "print(f'\\n{y}, \\nautomatically given a view of shape {y.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANCxuIU-FuR7",
        "outputId": "c33a3784-78ba-48c8-84f8-9d415b041f13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.9946, 0.8823, 0.5472, 0.9659],\n",
            "        [0.4512, 0.4721, 0.8671, 0.5193],\n",
            "        [0.4406, 0.0111, 0.4736, 0.6988],\n",
            "        [0.4123, 0.8340, 0.8374, 0.5683]])\n",
            "tensor([0.9946, 0.8823, 0.5472, 0.9659, 0.4512, 0.4721, 0.8671, 0.5193, 0.4406,\n",
            "        0.0111, 0.4736, 0.6988, 0.4123, 0.8340, 0.8374, 0.5683])\n",
            "tensor([[0.9946, 0.8823],\n",
            "        [0.5472, 0.9659],\n",
            "        [0.4512, 0.4721],\n",
            "        [0.8671, 0.5193],\n",
            "        [0.4406, 0.0111],\n",
            "        [0.4736, 0.6988],\n",
            "        [0.4123, 0.8340],\n",
            "        [0.8374, 0.5683]])\n",
            "\n",
            "tensor([[0.9946, 0.8823, 0.5472, 0.9659, 0.4512, 0.4721, 0.8671, 0.5193],\n",
            "        [0.4406, 0.0111, 0.4736, 0.6988, 0.4123, 0.8340, 0.8374, 0.5683]]), \n",
            "automatically given a view of shape torch.Size([2, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Creating a Tensor from a NumPy Array\n",
        "\n",
        "Transforming data between NumPy and PyTorch is a common practice.\n",
        "\n",
        "Typically, many calculations that do not require autograd, such as numerical\n",
        "simulations of physical environments, are done using NumPy. On the other hand,\n",
        "we use PyTorch when we need to leverage the power of autograd, computational\n",
        "graphs, and the ability to perform computations on GPUs for tasks like training\n",
        "models. These differences will become clearer as we explore how PyTorch aids in\n",
        "training models with large numbers of parameters in later tutorials.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        },
        "id": "xWpaSI8RQgxS",
        "outputId": "19f45602-fa48-4f41-b7ca-469e5c39948b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nCreating a Tensor from a NumPy Array\\n\\nTransforming data between NumPy and PyTorch is a common practice.\\n\\nTypically, many calculations that do not require autograd, such as numerical \\nsimulations of physical environments, are done using NumPy. On the other hand, \\nwe use PyTorch when we need to leverage the power of autograd and computational \\ngraphs for tasks like training models. These differences will become clearer as \\nwe explore how PyTorch aids in training models with large numbers of parameters \\nin later tutorials.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "a = torch.ones(5)\n",
        "print(a)\n",
        "\n",
        "# to transform a torch.tensor to np.array we simply do:\n",
        "b = a.numpy()\n",
        "print(type(a)) # for torch.tensor, you can also check it with a.dtype (return \"torch.float32\")\n",
        "print(type(b)) # but here, b.dtype simply return \"float32\"\n",
        "print(b)\n",
        "\n",
        "\"\"\"\n",
        "NOTE that if the data is located on the CPU (by default, they are on the CPU),\n",
        "both objects will share the same memory location.\n",
        "\n",
        "--> If we change one of them in-placed, another one will be changed too:\n",
        "\"\"\"\n",
        "a.add_(1)\n",
        "print(a)\n",
        "print(b)\n",
        "\n",
        "# Transform from np.array to torch.tensor:\n",
        "a = np.ones(5)\n",
        "print(a)\n",
        "b = torch.from_numpy(a)\n",
        "print(b)\n",
        "\n",
        "# Exercise: Try observing how torch.tensor and numpy.array data are printed\n",
        "\n",
        "\"\"\"\n",
        "Same here, they shared the same memory location:\n",
        "\"\"\"\n",
        "a += 1    # Note: a += 1 is in-place operation; a = a + 1 is out-of-place\n",
        "print(a)\n",
        "print(b)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "What if the data is on the GPU?\n",
        "\n",
        "To avoid confusion, we shall introduce how to move data from the CPU to the GPU later.\n",
        "Yet a simple fact is that NumPy cannot handle data on the GPU, so we always have\n",
        "to move torch.tensor back to the CPU before transforming it into NumPy object\n",
        "\"\"\"\n",
        "\n",
        "# Following is an example when the torch.tensor was initially on the GPU\n",
        "# Feel free to skip this part for the moment\n",
        "\n",
        "if torch.cuda.is_available(): # On Apple's M1 chip it will be False\n",
        "                              # On Google Colab, if you are not using T4 GPU it will be False too\n",
        "    device = torch.device(\"cuda\") # \"device\" object now represents GPU (CUDA stands for NVIDIA's GPUs)\n",
        "    x = torch.ones(5, device=device) # x is on \"cuda\" (GPU) now\n",
        "    # here doing x.numpy() will return error as NumPy can't handle data on GPU\n",
        "    x = x.to(\"cpu\") # moving x to cpu before transform\n",
        "    numpy_x = x.numpy()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQFDHk_xWlp-",
        "outputId": "1c97e2b4-e758-47fa-9cf5-5a3666bdf1a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 1., 1., 1., 1.])\n",
            "<class 'torch.Tensor'>\n",
            "<class 'numpy.ndarray'>\n",
            "[1. 1. 1. 1. 1.]\n",
            "tensor([2., 2., 2., 2., 2.])\n",
            "[2. 2. 2. 2. 2.]\n",
            "[1. 1. 1. 1. 1.]\n",
            "tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n",
            "[2. 2. 2. 2. 2.]\n",
            "tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "There are situations where we want to make sure the original NumPy data remains intact.\n",
        "In this case, we can directly use torch.tensor or torch.FloatTensor to create new copies\n",
        "instead of using `.from_numpy`, which shares memory with the original NumPy array.\n",
        "\"\"\"\n",
        "import torch\n",
        "import numpy as np\n",
        "a = np.array([1, 2, 3])\n",
        "print(a)\n",
        "\n",
        "b = torch.tensor(a, dtype=torch.float32)  # Creates a new copy\n",
        "c = torch.FloatTensor(a)  # Also creates a new copy\n",
        "print(b)\n",
        "b += 1\n",
        "c += 1\n",
        "\n",
        "print(a)  # NumPy array `a` remains unchanged\n",
        "print(b)  # Tensor `b` is modified\n",
        "print(c)  # Tensor `c` is modified\n",
        "\n",
        "\"\"\"\n",
        "Alternatively, `.clone()` is a commonly used method to make an independent copy of a tensor.\n",
        "\"\"\"\n",
        "\n",
        "b = torch.from_numpy(a)  # Shares memory with `a`\n",
        "# If we modify `b` now, the values in `a` will also be affected.\n",
        "b = b.clone()  # We make a copy and assign it to `b`\n",
        "b += 1\n",
        "\n",
        "print(a)  # `a` is not influenced after cloning\n",
        "print(b)  # `b` is modified independently\n",
        "\n",
        "# Note: just keep in mind first here that the .clone() method do preserve the computational graph.\n",
        "# For NumPy array, the function to make a copy is `.copy()`, e.g., try a.copy().\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "de5JTkRZp0Rx",
        "outputId": "74bdb034-d2c6-4c35-db62-6cab73032e5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 2 3]\n",
            "tensor([1., 2., 3.])\n",
            "[1 2 3]\n",
            "tensor([2., 3., 4.])\n",
            "tensor([2., 3., 4.])\n",
            "[1 2 3]\n",
            "tensor([2, 3, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# From the next tutorial, we will start exploring the power of PyTorch tensors. How can we leverage this power? What sets PyTorch apart from just using NumPy?\n"
      ],
      "metadata": {
        "id": "oiqr_nXJdnyk"
      }
    }
  ]
}