{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In this tutorial, we will integrate what we’ve learned by building a basic image classification model. Our task will be to classify handwritten digits using the famous MNIST dataset.\n",
        "\n",
        " * Note: We will also explore how to utilize a GPU to accelerate training.\n",
        "\n",
        " *Reference video: https://www.youtube.com/watch?v=oPhxf2fXHkQ&list=PLqnslRFeH2UrcDBWF5mfPGpqQDSta6VK4&index=14*"
      ],
      "metadata": {
        "id": "7SE6TaLUDa_w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GU0F9gI7_Tuw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        },
        "outputId": "06afce32-fe38-40d5-a6b9-fe469abe947e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([100, 1, 28, 28]) torch.Size([100])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nreturn torch.Size([100, 1, 28, 28]) torch.Size([100]) \\n\\ntorch.Size([100, 1, 28, 28]) -> 100 for the asked batch size, 1 for black and white channel (no color channel)\\nthe last two 28x28 are the pixel array. \\n\\nthe torch.Size([100]) are the label of data. Each data in the batch has a label (number 0 - 9),\\nso here we have 100 return label corresponding to the 100 data in the batch\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision # for the MNIST dataset, images of hand-written digits\n",
        "import torchvision.transforms as transforms # for organizing the MNIST dataset\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# device config, use GPU if there is one, else CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# hyper parameters\n",
        "input_size = 784 # 28x28 -> the image dataset we will use containing images with 28*28 pixels\n",
        "hidden_size = 100\n",
        "num_classes = 10 # the output size, there are 10 possible output from 0 - 9\n",
        "num_epochs = 2\n",
        "batch_size = 100\n",
        "learning_rate = 0.001\n",
        "\"\"\"\n",
        "- Epoch:        An epoch refers to one complete pass through the entire training dataset.\n",
        "                In each epoch, the model sees all the training examples once.\n",
        "\n",
        "- Batch:        Instead of passing the entire dataset to the model at once, we divide it into smaller subsets called batches.\n",
        "\n",
        "- Iteration:    An iteration refers to a single update of the model’s parameters.\n",
        "                In one iteration, the model processes one batch of data.\n",
        "\n",
        "--> If we have 1000 images and a batch size of 100, we’ll have 10 batches in each epoch, hence 10 iterations in each epoch\n",
        "\n",
        "Reference of the relations between epochs, batches, and iterations: https://www.youtube.com/watch?v=K20lVDVjPn4\n",
        "\"\"\"\n",
        "\n",
        "#### Loading MNIST (from PyTorch library)\n",
        "train_dataset = torchvision.datasets.MNIST(root='./data', train=True,\n",
        "                                           transform=transforms.ToTensor(), download=True) # download=True for the first time to download this dataset\n",
        "\n",
        "test_dataset = torchvision.datasets.MNIST(root='./data', train=False,\n",
        "                                          transform=transforms.ToTensor())\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=batch_size, shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=batch_size, shuffle=False)\n",
        "\"\"\"\n",
        "PyTorch provides built-in `DataLoader` and `transforms` to efficiently organize and process large datasets in common formats.\n",
        "While these tools are not strictly necessary, they are no doubt convenient when working with pre-existing datasets, as they\n",
        "handle batching, shuffling, and data transformations easily.\n",
        "\n",
        "However, we won’t focus on these packages in this tutorial, as they are more relevant for tasks involving pre-existing datasets,\n",
        "such as image classification. For tasks like Reinforcement Learning or functional optimization problems, which we are focusing on,\n",
        "most data are generated dynamically during training. In such cases, the `DataLoader` and `transforms` functionalities are less\n",
        "useful since we work with training data that is created on the fly.\n",
        "\n",
        "To learn the basics of PyTorch `DataLoader` and `transforms`, the following references are recommended:\n",
        "https://www.youtube.com/watch?v=PXOzkkB5eH0&list=PLqnslRFeH2UrcDBWF5mfPGpqQDSta6VK4&index=9\n",
        "https://www.youtube.com/watch?v=X_QOZEko5uE&list=PLqnslRFeH2UrcDBWF5mfPGpqQDSta6VK4&index=10\n",
        "\n",
        "It's also worth noting that if you're working on both PyTorch and TensorFlow projects simultaneously, you might prefer to organize\n",
        "datasets using interchangeable Python code (such as `NumPy` or `Pandas`). This allows for greater flexibility across both frameworks,\n",
        "as it avoids locking data loading into PyTorch's `DataLoader` or TensorFlow's `tf.data` pipeline. By keeping dataset handling\n",
        "framework-agnostic, you can easily adapt your code for use in both environments.\n",
        "\"\"\"\n",
        "#### Loading MNIST ends\n",
        "\n",
        "examples = iter(train_loader)\n",
        "samples, labels = next(examples)  # In Python 3.x, use the built-in next() function instead of examples.next()\n",
        "print(samples.shape, labels.shape)\n",
        "\"\"\"\n",
        "return torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
        "\n",
        "torch.Size([100, 1, 28, 28]) -> 100 for the asked batch size, 1 for black and white channel (no color channel)\n",
        "the last two 28x28 are the pixel array.\n",
        "\n",
        "the torch.Size([100]) are the labels of data. Each data in the batch has a label (number 0 - 9),\n",
        "so here we have 100 return labels corresponding to the 100 data in the batch\n",
        "\n",
        "torch.Size([100, 1, 28, 28]):\n",
        "- The first dimension (100) represents the batch size, which we set to 100. (loaded with dataloader)\n",
        "- The second dimension (1) represents the number of channels. Since these are black-and-white images, we only have 1 channel (no color channels like RGB).\n",
        "- The last two dimensions (28x28) represent the height and width of the image in pixels, corresponding to the 28x28 pixel array.\n",
        "\n",
        "torch.Size([100]):\n",
        "- This represents the labels of the data. Since each data sample in the batch has one label (a digit from 0 to 9), we have 100 labels corresponding to the 100 images in the batch.\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's take a look at the first 6 images in our samples\n",
        "for i in range(6):\n",
        "    plt.subplot(2, 3, i+1)\n",
        "    plt.imshow(samples[i][0], cmap='gray')\n",
        "    # The [0] is to access the color channel (black and white) for the (28, 28) image\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 580
        },
        "id": "Npwxr7qtGzG4",
        "outputId": "9bac741f-b703-4543-8d58-27eade9fb07c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function matplotlib.pyplot.show(close=None, block=None)>"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>matplotlib.pyplot.show</b><br/>def show(*args, **kwargs)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/matplotlib/pyplot.py</a>Display all open figures.\n",
              "\n",
              "Parameters\n",
              "----------\n",
              "block : bool, optional\n",
              "    Whether to wait for all figures to be closed before returning.\n",
              "\n",
              "    If `True` block and run the GUI main loop until all figure windows\n",
              "    are closed.\n",
              "\n",
              "    If `False` ensure that all figure windows are displayed and return\n",
              "    immediately.  In this case, you are responsible for ensuring\n",
              "    that the event loop is running to have responsive figures.\n",
              "\n",
              "    Defaults to True in non-interactive mode and to False in interactive\n",
              "    mode (see `.pyplot.isinteractive`).\n",
              "\n",
              "See Also\n",
              "--------\n",
              "ion : Enable interactive mode, which shows / updates the figure after\n",
              "      every plotting command, so that calling ``show()`` is not necessary.\n",
              "ioff : Disable interactive mode.\n",
              "savefig : Save the figure to an image file instead of showing it on screen.\n",
              "\n",
              "Notes\n",
              "-----\n",
              "**Saving figures to file and showing a window at the same time**\n",
              "\n",
              "If you want an image file as well as a user interface window, use\n",
              "`.pyplot.savefig` before `.pyplot.show`. At the end of (a blocking)\n",
              "``show()`` the figure is closed and thus unregistered from pyplot. Calling\n",
              "`.pyplot.savefig` afterwards would save a new and thus empty figure. This\n",
              "limitation of command order does not apply if the show is non-blocking or\n",
              "if you keep a reference to the figure and use `.Figure.savefig`.\n",
              "\n",
              "**Auto-show in jupyter notebooks**\n",
              "\n",
              "The jupyter backends (activated via ``%matplotlib inline``,\n",
              "``%matplotlib notebook``, or ``%matplotlib widget``), call ``show()`` at\n",
              "the end of every cell by default. Thus, you usually don&#x27;t have to call it\n",
              "explicitly there.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 401);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 6 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define our Neural Network\n",
        "class NeuralNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(NeuralNet, self).__init__()\n",
        "        self.l1 = nn.Linear(input_size, hidden_size)\n",
        "        self.activationF = nn.ReLU() # different from tutorial5, here we try to apply the activation function as a module\n",
        "        self.l2 = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.l1(x)\n",
        "        x = self.activationF(x)\n",
        "        output = self.l2(x)\n",
        "        return output\n",
        "\n",
        "# Instantiate our Neural Network\n",
        "model = NeuralNet(input_size, hidden_size, num_classes)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()  # Cross-entropy loss for classification\n",
        "\"\"\"\n",
        "The loss function, also known as the criterion, again, is a measure of how well the model's predictions match the target labels.\n",
        "In the previous tutorial, we defined the loss function (Mean Squared Error) manually.\n",
        "However, PyTorch provides built-in loss functions, such as:\n",
        "- `nn.MSELoss()` for regression problems.\n",
        "- `nn.CrossEntropyLoss()` for classification problems, which we are using here.\n",
        "\n",
        "**CrossEntropyLoss** is commonly used for classification tasks where the model outputs probabilities over multiple classes.\n",
        "It compares the predicted class probabilities with the true class labels and penalizes incorrect predictions.\n",
        "\n",
        "In our case, since we are classifying digits (0-9), CrossEntropyLoss is well-suited because it handles multi-class classification efficiently.\n",
        "\n",
        "To learn more about loss functions (e.g. Softmax and Cross-Entropy) used in classification problems, please check:\n",
        "https://www.youtube.com/watch?v=7q7E91pHoW4&list=PLqnslRFeH2UrcDBWF5mfPGpqQDSta6VK4&index=11\n",
        "\n",
        "To explore different built-in PyTorch loss functions, visit the documentation:\n",
        "https://pytorch.org/docs/stable/nn.html#loss-functions\n",
        "\"\"\"\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  # Adam optimizer\n",
        "# Adam: https://pytorch.org/docs/stable/generated/torch.optim.Adam.html\n",
        "\n",
        "\n",
        "# Main training loop\n",
        "n_total_steps = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # Flatten the 28x28 images into a vector of 784 pixels\n",
        "        # [100, 1, 28, 28] -> [100, 784]\n",
        "        # Send the flattened images and labels to the chosen device (GPU if available)\n",
        "        images = images.reshape(-1, 28*28).to(device)\n",
        "        labels = labels.to(device) # remember to send the labels to device too! (they are part of the backward pass)\n",
        "        # The \"-1\" in reshape automatically calculates the batch size (here should be 100) based on the input dimension\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print training info every 100 steps\n",
        "        if (i+1) % 100 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# testing and evaluation our result\n",
        "with torch.no_grad(): # not for training, no need of computational graph\n",
        "    n_correct = 0\n",
        "    n_samples = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.reshape(-1, 28*28).to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "\n",
        "        # Get the predicted class (index of the highest value in the output tensor)\n",
        "        _, predictions = torch.max(outputs, 1)\n",
        "        # torch.max() returns (max value, index of max value). We use the index to get the predicted class.\n",
        "\n",
        "\n",
        "        n_samples += labels.shape[0] # return the number of samples in the current batch (100)\n",
        "        n_correct += (predictions == labels).sum().item()\n",
        "        # if predictions == labels, it will return True, which is also a value \"1\".\n",
        "\n",
        "    # Calculate accuracy as a percentage\n",
        "    acc = 100.0 * n_correct / n_samples\n",
        "    print(f'accuracy = {acc}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1vO-owfJt4a",
        "outputId": "d0a89a26-34c1-43dd-d460-a5d2adf7d3fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1 / 2, step 100/600, loss 0.4461\n",
            "epoch 1 / 2, step 200/600, loss 0.3122\n",
            "epoch 1 / 2, step 300/600, loss 0.4404\n",
            "epoch 1 / 2, step 400/600, loss 0.2136\n",
            "epoch 1 / 2, step 500/600, loss 0.3662\n",
            "epoch 1 / 2, step 600/600, loss 0.1354\n",
            "epoch 2 / 2, step 100/600, loss 0.1791\n",
            "epoch 2 / 2, step 200/600, loss 0.2808\n",
            "epoch 2 / 2, step 300/600, loss 0.1647\n",
            "epoch 2 / 2, step 400/600, loss 0.1218\n",
            "epoch 2 / 2, step 500/600, loss 0.2144\n",
            "epoch 2 / 2, step 600/600, loss 0.1981\n",
            "accuracy = 95.52\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#About flattening the data:\n",
        "\n",
        "In PyTorch, both `reshape()` and `view()` are used to change the shape of a tensor, but they differ in how they operate and the conditions in which they can be used:\n",
        "\n",
        "1. `view()`\n",
        "\n",
        "\t•\tOperation: `view()` returns a new tensor with the same data but a different shape. However, it requires that the tensor be contiguous in memory. If the tensor is not contiguous, you may encounter an error.\n",
        "    \n",
        "\t•\tContiguous Memory: PyTorch stores tensors in memory as contiguous blocks, but certain operations (like slicing or transposing) can cause the data to be stored in a non-contiguous manner. `view()` can only be used on tensors that are contiguous in memory. You can use `.contiguous()` to ensure a tensor is contiguous before using `view()`.\n",
        "\n",
        "When to Use `view()`:\n",
        "\n",
        "\t•\tWhen you want to change the shape of a tensor, and you are sure or can make the tensor contiguous.\n",
        "\t•\tUseful for reshaping without creating a new copy of the tensor if the underlying data layout allows it.\n",
        "\n",
        "2. `reshape()`\n",
        "\n",
        "\t•\tOperation: `reshape()` also returns a tensor with a different shape, but it does not require the tensor to be contiguous. If the tensor is not contiguous, `reshape()` will internally make a copy of the data if needed. This provides more flexibility but might be slightly less efficient since it can involve copying data.\n",
        "\n",
        "\t•\tMemory Safety: `reshape()` is generally more flexible since it will handle both contiguous and non-contiguous tensors.\n",
        "\n",
        "When to Use `reshape()`:\n",
        "\n",
        "\t•\tWhen you need to reshape a tensor and don’t want to worry about whether the tensor is contiguous or not.\n",
        "\t•\tWhen you prefer flexibility and don’t mind the potential overhead of creating a new tensor."
      ],
      "metadata": {
        "id": "E0w4NVKwTha7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this tutorial, we successfully built a simple neural network to classify images from the MNIST dataset. While this fully connected network performs well on simple tasks like digit classification, more complex image processing tasks often require more sophisticated architectures.\n",
        "\n",
        "For most advanced computer vision tasks, such as object detection, image segmentation, or more challenging classification tasks, **Convolutional Neural Networks (CNNs)** are essential for improving performance. CNNs are specifically designed to capture spatial hierarchies in images, which makes them much more effective for image-related tasks.\n",
        "\n",
        "If you're interested in learning more about CNNs and their application in deep learning, I recommend checking out this YouTube video: https://www.youtube.com/watch?v=pDdP0TFzsoQ&list=PLqnslRFeH2UrcDBWF5mfPGpqQDSta6VK4&index=14\n"
      ],
      "metadata": {
        "id": "U8bBufLvXbKW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uKrkXTBkKi1c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
