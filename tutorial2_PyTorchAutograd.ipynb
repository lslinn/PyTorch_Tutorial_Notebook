{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# In this section, we will begin exploring PyTorch's powerful capability of automatically tracking gradients through its computational graph. Imagine using gradient descent to update a neural network model with countless parameters. PyTorch's autograd system records the computational graph during the forward pass and systematically computes all gradients during the backward pass.\n",
        "\n",
        "*Reference video: https://www.youtube.com/watch?v=DbeIqrwb_dE&list=PLqnslRFeH2UrcDBWF5mfPGpqQDSta6VK4&index=3*"
      ],
      "metadata": {
        "id": "kqHhKJY0uQd3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVOLLHfFt2dd"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's start with a very simple regression problem\n",
        "\n",
        "x = torch.randn(3)\n",
        "print(x)\n",
        "\n",
        "\"\"\"\n",
        "Let's consider that this `x` is a parameter of some model.\n",
        "Our aim is to update this `x` to make better predictions.\n",
        "\n",
        "Let's first apply a simple check:\n",
        "\"\"\"\n",
        "\n",
        "print(x.requires_grad)\n",
        "# This returns False.\n",
        "\n",
        "\"\"\"\n",
        "`.requires_grad` is an attribute in PyTorch that indicates whether a tensor requires gradient tracking.\n",
        "In other words, it determines whether this tensor is part of the computational graph for later gradient computation.\n",
        "\n",
        "The return value of False simply indicates that `x` is not being tracked for gradients yet.\n",
        "To enable gradient tracking, we can do the following:\n",
        "\"\"\"\n",
        "\n",
        "x.requires_grad = True\n",
        "print(x.requires_grad)\n",
        "\n",
        "\"\"\"\n",
        "Alternatively, another simple way to achieve this when creating the tensor is:\n",
        "\"\"\"\n",
        "x = torch.randn(3, requires_grad=True)\n",
        "print(x.requires_grad)\n",
        "\n",
        "# Now, x is ready for gradient tracking and can participate in the computational graph.\n",
        "\n",
        "\"\"\"\n",
        "Let us now perform a very simple forward pass (where the forward pass refers to\n",
        "how we let the input data flow through, or the series of operations we apply to the input).\n",
        "\"\"\"\n",
        "\n",
        "y = x + 2\n",
        "z = y * y * 2\n",
        "z = z.mean()\n",
        "\n",
        "\"\"\"\n",
        "The forward pass (computational graph) can be described as:\n",
        "x --> addition operation (+ 2) --> output y --> multiplication operation (y * y * 2)\n",
        "--> output z --> mean operation --> final output\n",
        "\"\"\"\n",
        "print(z)  # Note that the printed `z` displays the latest operation: grad_fn=<MeanBackward0>\n",
        "\n",
        "\"\"\"\n",
        "Exercise: Besides <MeanBackward0>, what are the `grad_fn` values for other types of operations?\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Now, we can apply the built-in function `.backward()` to compute the gradients of `z` with respect\n",
        "to the `requires_grad=True` parameters recorded in the computational graph. PyTorch automatically tracks\n",
        "the backward path through the computational graph to calculate all these gradients. This process is known as\n",
        "\"backpropagation.\"\n",
        "\n",
        "IMPORTANT: Intermediate variables, like `y` in this case, are automatically created with `requires_grad=True`\n",
        "when they are the result of operations involving other `requires_grad=True` variables, such as `x`.\n",
        "\n",
        "Note: The gradient computation in PyTorch is based on the \"vector-Jacobian product\" technique.\n",
        "This means if `z` is a scalar (a single value), PyTorch can compute the gradients directly. However,\n",
        "if `z` is not a scalar (i.e., it is a tensor with multiple values), you must provide a vector to the `.backward()`\n",
        "function that represents how the gradients should be weighted (this is typically referred to as the `gradient` argument).\n",
        "Explanation of this Jacobian process in neural network, see: https://www.youtube.com/watch?v=AdV5w8CY3pw&t=3s\n",
        "\"\"\"\n",
        "\n",
        "# To find out ∂z / ∂x, we simply do:\n",
        "z.backward()\n",
        "print(x.grad)  # Gradient of z with respect to x\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "veT_PXuHwADq",
        "outputId": "ac995a0a-2ba6-4de4-a9d0-73f980ddfba8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-0.6120,  0.5023, -0.3509])\n",
            "False\n",
            "True\n",
            "True\n",
            "tensor(15.6566, grad_fn=<MeanBackward0>)\n",
            "tensor([5.2619, 3.5785, 1.1214])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "The following points (related to \"leaf variables\") go beyond what we need to know for now, but can be important:\n",
        "(feel free to skip if you are not ready)\n",
        "\n",
        "In a nutshell, the tensors we MANUALLY create with `requires_grad=True` are called \"leaf variables.\"\n",
        "On the other hand, `y` here is an intermediate variable created through an operation.\n",
        "Although `y` has `requires_grad=True` because it is part of the computational graph, it is not a \"leaf variable.\"\n",
        "When a tensor is not a leaf variable, PyTorch does not store its gradient during `backward()` to save memory.\n",
        "This makes sense since we typically do not need to update these intermediate variables.\n",
        "\n",
        "If, for some reason, you need gradients for such intermediate variables, you can use `retain_grad()` to ensure\n",
        "the gradient is stored during backpropagation.\n",
        "\"\"\"\n",
        "\n",
        "# Check gradients and leaf status\n",
        "print(y.grad)  # You will find ∂z / ∂y is not recorded and simply returns None\n",
        "print(x.is_leaf)  # x is a leaf variable\n",
        "print(y.is_leaf)  # y is not a leaf variable\n",
        "\n",
        "# Using retain_grad() to retain gradients for the intermediate variable y\n",
        "y = x + 2\n",
        "z = y * y * 2\n",
        "z = z.mean()\n",
        "\n",
        "y.retain_grad()\n",
        "z.backward()\n",
        "\n",
        "# Now the gradient on y is also retained\n",
        "print(f'Now the grad on y is also retained: {y.grad}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GPAr122uHRW",
        "outputId": "071d6aeb-d972-48ad-e76c-0b1024dfed02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "True\n",
            "False\n",
            "Now the grad on y is also retained: tensor([5.2619, 3.5785, 1.1214])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-a49535cc7498>:16: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:489.)\n",
            "  print(y.grad)  # You will find ∂z / ∂y is not recorded and simply returns None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "IMPORTANT:\n",
        "\n",
        "When calling `.backward()`, the gradients are recorded (also called \"populated\"),\n",
        "but at the same time, the involved \"forward computational graph\" is freed from memory.\n",
        "This means an error will occur if you try to call `z.backward()` again without recomputing\n",
        "the forward pass (i.e., `y = x + 2; z = y * y * 2; z = z.mean()`).\n",
        "\n",
        "To prevent the system from freeing the computational graph during the backward pass, you can use\n",
        "`z.backward(retain_graph=True)`. However, this is for more advanced use cases, and we will discuss its\n",
        "applications in a later tutorial.\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83
        },
        "id": "G75r07cLESmr",
        "outputId": "54d33969-68ac-433a-92d7-9f13281145f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nIMPORTANT:\\n\\nWhen calling `.backward()`, the gradients are recorded (also called \"populated\"), \\nbut at the same time, the involved \"forward computational graph\" is freed from memory. \\nThis means an error will occur if you try to call `z.backward()` again without recomputing \\nthe forward pass (i.e., `y = x + 2; z = y * y * 2; z = z.mean()`).\\n\\nTo prevent the system from freeing the computational graph during the backward pass, you can use\\n`z.backward(retain_graph=True)`. However, this is for more advanced use cases, and we will discuss its\\napplications in a later tutorial.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Another important characteristic of PyTorch's `.backward()` function is that,\n",
        "when calling this function, the calculated gradients are \"added\" to the `requires_grad` variables.\n",
        "Therefore, if those variables already have gradient information from previous operations, calling\n",
        "`.backward()` will simply add the newly calculated gradients to the existing ones.\n",
        "\n",
        "Here’s an example of what could happen:\n",
        "\"\"\"\n",
        "\n",
        "# Example\n",
        "weights = torch.ones(4, requires_grad=True)\n",
        "\n",
        "model_output = (weights * 3).sum()\n",
        "model_output.backward()\n",
        "print(weights.grad)  # The gradient is calculated and printed\n",
        "\n",
        "# Now, let's try calling .backward() again.\n",
        "# Remember, before we do another backward pass, we need to recompute the forward pass\n",
        "# because the previous computational graph was freed.\n",
        "model_output = (weights * 3).sum()\n",
        "model_output.backward()\n",
        "print(weights.grad)\n",
        "\n",
        "\"\"\"\n",
        "The gradient has now become [6, 6, 6, 6], as this is the second time we called `.backward()`.\n",
        "This happens because PyTorch accumulates gradients, adding the new ones to the existing gradients.\n",
        "If the operation was just `(weights * 3).sum()` once, the gradient would only be [3, 3, 3, 3].\n",
        "\n",
        "In some situations, this characteristic is useful if you want to accumulate gradients while backwarding\n",
        "through different losses step by step.\n",
        "\n",
        "However, in this case, to ensure the gradients are correct for each backward pass, we need to zero out\n",
        "the previous gradients before calling `.backward()` again:\n",
        "\"\"\"\n",
        "\n",
        "weights.grad.zero_()  # Zero out the gradients\n",
        "print(weights.grad)  # The gradients are now reset to zero\n",
        "\n",
        "# Hence, for multiple iterations:\n",
        "for epoch in range(5):\n",
        "    weights.grad.zero_()  # Zero out gradients each time\n",
        "    model_output = (weights * 3).sum()\n",
        "    model_output.backward()\n",
        "\n",
        "print(weights.grad)\n",
        "\n",
        "# No matter how many iterations we apply, the gradients are correct now."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6HhX1wg_Yus",
        "outputId": "3232b775-3da4-491a-a449-9b839338d3c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([3., 3., 3., 3.])\n",
            "tensor([6., 6., 6., 6.])\n",
            "tensor([0., 0., 0., 0.])\n",
            "tensor([3., 3., 3., 3.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "In some situations, we may not want PyTorch to track gradients.\n",
        "For example, when we are testing our model, or when we need some intermediate\n",
        "quantities in the computational graph but want to cut their relation with the original graph.\n",
        "\n",
        "In these situations, there are three ways to stop PyTorch from tracking gradients:\n",
        "1. `x.requires_grad_(False)` or `x.requires_grad = False`\n",
        "2. `x.detach()`\n",
        "3. Wrapping the operations in `with torch.no_grad():`\n",
        "\"\"\"\n",
        "\n",
        "# For the 1st way\n",
        "x = torch.randn(3, requires_grad=True)\n",
        "print(x)  # x requires gradients here\n",
        "\n",
        "# Now we can choose to call the function\n",
        "x.requires_grad_(False)\n",
        "print(x)  # x doesn't require gradients anymore\n",
        "\n",
        "# Similarly, we can modify the attribute directly\n",
        "x = torch.randn(3, requires_grad=True)\n",
        "x.requires_grad = False\n",
        "print(x)  # x doesn't require gradients\n",
        "\n",
        "\"\"\"\n",
        "Setting `requires_grad=False` directly and calling `requires_grad_(False)` are equivalent.\n",
        "The key difference is that `requires_grad` is an \"attribute,\" while `requires_grad_()` is a function.\n",
        "Note that `requires_grad_()` is an in-place function, as indicated by the trailing underscore.\n",
        "\"\"\"\n",
        "\n",
        "# For the 2nd way\n",
        "x = torch.randn(3, requires_grad=True)\n",
        "print(x)  # x requires gradients\n",
        "y = x.detach()\n",
        "print(y)  # y is a copy of x which doesn't require gradients\n",
        "\n",
        "\"\"\"\n",
        "The critical difference between `.detach()` and `requires_grad_()` is that `.detach()` creates\n",
        "a new tensor `y` that shares the same data as `quantity1` but doesn't track gradients.\n",
        "It is an out-of-place operation, meaning `quantity1` remains intact in the computational graph while `y` is independent.\n",
        "\n",
        "Example of using `.detach()`:\n",
        "\n",
        "input -> operation1 -> quantity1 -> operation2 -> quantity2 -> loss\n",
        "                            |\n",
        "                            --> y = quantity1.detach() -> (operation on y not for backward) -> some information wanted\n",
        "\n",
        "In this example, detaching `quantity1` allows you to perform additional operations on `y` without affecting the backward\n",
        "pass for the loss. The original `quantity1` remains part of the computational graph.\n",
        "\n",
        "Note: If you use `quantity1.requires_grad = False`, the backward pass (`backward(loss)`) will not raise an error,\n",
        "but `quantity1` will be disconnected from the graph, and no gradients will be computed for it or any earlier computations\n",
        "involving it. In contrast, `.detach()` keeps the computational graph intact for `quantity1`.\n",
        "\"\"\"\n",
        "\n",
        "# For the 3rd way\n",
        "x = torch.randn(3, requires_grad=True)\n",
        "with torch.no_grad():\n",
        "    y = x + 2\n",
        "    print(y)  # y doesn't require gradients\n",
        "\n",
        "\"\"\"\n",
        "The `torch.no_grad()` context is commonly used when testing a model or when passing input\n",
        "through a neural network without wanting to track the computations in the computational graph.\n",
        "This prevents gradient information from being stored and speeds up computations.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "45EuAnaIEaa1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc60b246-48a7-4e78-8e13-3627ee02608e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-0.9308,  0.7550,  0.9499], requires_grad=True)\n",
            "tensor([-0.9308,  0.7550,  0.9499])\n",
            "tensor([ 0.8912,  1.7899, -1.6068])\n",
            "tensor([-1.7852, -0.9613,  0.9818], requires_grad=True)\n",
            "tensor([-1.7852, -0.9613,  0.9818])\n",
            "tensor([2.5161, 0.6254, 1.2002])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1_9H710c8MTy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}